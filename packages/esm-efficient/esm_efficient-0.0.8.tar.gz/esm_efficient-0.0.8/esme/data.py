import polars as pl
import sklearn
import lightning as L
import sklearn.impute
import sklearn.utils
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
from esme.alphabet import tokenize, pad_tokens, mask_tokens, tokenize_unpad, Alphabet3


def read_fai(fai_path):
    '''
    Read a fasta index file (.fai) into a DataFrame. The index file is
    generated by `samtools faidx` command.

    Args:
        fai_path (str): Path to the fasta index file.

    Returns:
        pl.DataFrame: polars DataFrame with the following columns:
            - id: Sequence ID.
            - length: Sequence length.
            - offset: Byte offset of the sequence in the fasta file.
            - line_bases: Number of bases per line.
            - line_width: Number of characters per line.
    '''
    cols = ["id", "length", "offset", "line_bases", "line_width"]
    return pl.read_csv(fai_path, separator="\t", has_header=False) \
        .rename({f'column_{i + 1}': col for i, col in enumerate(cols)})


class TokenSizeBatchSampler:
    '''
    Batch sampler based on the token size of the sequences in the dataset.

    Args:
        token_sizes (list): List of token sizes for each sequence in the dataset.
        token_per_batch (int): Maximum number of tokens per batch.
        drop_last (bool): If True, drop the last batch if it is smaller than
            `token_per_batch`.
    '''

    def __init__(self, token_sizes, token_per_batch, drop_last=False,
                 shuffle=True, random_state=None):
        self.token_sizes = token_sizes
        self.token_per_batch = token_per_batch
        self.drop_last = drop_last
        self.shuffle = shuffle
        self.random_state = random_state
        self._batches = list(self.batches())

    def batches(self):
        indices = list(range(len(self.token_sizes)))

        if self.shuffle:
            indices = sklearn.utils.shuffle(
                indices, random_state=self.random_state)

        batch = list()
        max_len = 0

        for idx in indices:
            token_len = self.token_sizes[idx] + 2  # add 2 for start, end

            if max_len + token_len > self.token_per_batch:
                yield batch
                max_len = token_len
                batch = [idx]
            else:
                max_len += token_len
                batch.append(idx)

        if len(batch) > 0 and (not self.drop_last):
            yield batch

    def __getitem__(self, idx):
        return self._batches[idx]

    def __len__(self):
        return len(self._batches)


class BaseFastaDataset(Dataset):

    def __init__(self, fasta, fai=None, k_sample=None, max_len=None, alphabet=Alphabet3):
        self.max_len = max_len or float('inf')
        self.fasta = fasta
        self.alphabet = alphabet

        if fai is None:
            fai = fasta + ".fai"

        try:
            self.fai = read_fai(fai)
        except FileNotFoundError as e:
            raise FileNotFoundError(
                f"Please index the fasta file with samtools faidx: "
                f"`samtools faidx {fasta}`"
            ) from e

        if k_sample:
            self.fai = self.fai.sample(k_sample)

        if max_len is not None:
            self.fai = self.fai.filter(pl.col("length") <= max_len)

    def read_seq(self, idx):
        _, length, offset, _, _ = self.fai.row(idx)

        with open(self.fasta) as f:
            f.seek(offset)

            lines = list()

            while True:
                line = f.readline()
                if line.startswith('>') or line == '':
                    break
                lines.append(line.strip())

            seq = "".join(lines)
            assert len(seq) == length

        return seq

    def __len__(self):
        raise NotImplementedError()

    def __getitem__(self, idx):
        raise NotImplementedError()


class FastaDataset(BaseFastaDataset):
    '''
    Dataset for reading fasta files into tokenized sequences.

    Args:
        fasta (str): Path to the fasta file.
        fai (str, optional): Path to the fasta index file. If not provided,
            it is assumed to be `fasta + ".fai"`.
        k_sample (int, optional): Number of sequences to sample from the fasta
            file. If None, all sequences are used.
        max_len (int, optional): Maximum length of the sequences to include.
            If None, all sequences are used.
    '''

    def __init__(self, fasta, fai=None, k_sample=None, max_len=None, alphabet=Alphabet3):
        super().__init__(fasta, fai=fai, k_sample=k_sample,
                         max_len=max_len, alphabet=alphabet)

    def __len__(self):
        return len(self.fai)

    def __getitem__(self, idx):
        return tokenize(self.read_seq(idx), alphabet=self.alphabet)

    @staticmethod
    def collate_fn(batch):
        return pad_tokens(batch)

    def to_dataloader(self, batch_size, shuffle=False, num_workers=0, **kwargs):
        return DataLoader(self, batch_size=batch_size, shuffle=shuffle,
                          num_workers=num_workers, collate_fn=self.collate_fn,
                          **kwargs)


class FastaTokenDataset(BaseFastaDataset):
    '''
    Dataset for reading fasta files into tokenized sequences with a fixed
    number of tokens per batch.

    Args:
        fasta (str): Path to the fasta file.
        fai (str, optional): Path to the fasta index file. If not provided,
            it is assumed to be `fasta + ".fai"`.
        token_per_batch (int): Maximum number of tokens per batch.
        k_sample (int, optional): Number of sequences to sample from the fasta
            file. If None, all sequences are used.
        max_len (int, optional): Maximum length of the sequences to include.
            If None, all sequences are used.
        drop_last (bool): If True, drop the last batch if it is smaller than
            `token_per_batch`.
        shuffle (bool): If True, shuffle the sequences.
        random_state (int): Random seed for shuffling the sequences.
    '''

    def __init__(self, fasta, fai=None, token_per_batch=50_000, k_sample=None,
                 max_len=None, drop_last=False, shuffle=True, random_state=None, alphabet=Alphabet3):
        super().__init__(fasta, fai=fai, k_sample=k_sample, max_len=max_len)
        self.token_per_batch = token_per_batch
        self.alphabet = alphabet

        self.sampler = list(iter(TokenSizeBatchSampler(
            self.fai['length'].to_list(), token_per_batch, drop_last=drop_last,
            shuffle=shuffle, random_state=random_state
        )))

    def __len__(self):
        return len(self.sampler)

    def __getitem__(self, idx):
        indices = self.sampler[idx]
        token, _, cu_lens, max_len = tokenize_unpad(
            [self.read_seq(i) for i in indices], alphabet=self.alphabet)
        return token, (cu_lens, max_len)

    def to_dataloader(self, num_workers=0, **kwargs):
        return DataLoader(
            self,
            num_workers=num_workers,
            batch_size=None,  # batch_size is calculated by TokenSizeBatchSampler
            **kwargs
        )


class MaskedFastaDataset(FastaDataset):
    '''
    Dataset for reading fasta files into tokenized sequences with masked tokens.

    Args:
        fasta (str): Path to the fasta file.
        fai (str, optional): Path to the fasta index file. If not provided,
            it is assumed to be `fasta + ".fai"`.
        k_sample (int, optional): Number of sequences to sample from the fasta
            file. If None, all sequences are used.
        max_len (int, optional): Maximum length of the sequences to include.
            If None, all sequences are used.
        mask_freq (float): Frequency of masked tokens.
        alter_freq (float): Frequency of altered
    '''

    def __init__(self, fasta, fai=None, max_len=None, k_sample=None, mask_freq=.15,
                 alter_freq=.1, alphabet=Alphabet3):
        super().__init__(fasta, fai=fai, k_sample=k_sample,
                         max_len=max_len, alphabet=Alphabet3)
        self.mask_freq = mask_freq
        self.alter_freq = alter_freq

    def __getitem__(self, idx):
        token = super().__getitem__(idx)
        mtokens, mask = mask_tokens(token, self.mask_freq, self.alter_freq,
                                    alphabet=self.alphabet)
        return token, mtokens, mask

    @staticmethod
    def collate_fn(batch):
        tokens = pad_tokens([i[0] for i in batch])
        mtokens = pad_tokens([i[1] for i in batch])
        mask = pad_sequence([i[2][0] for i in batch], batch_first=True,
                            padding_value=False)
        return tokens, mtokens, mask

    def to_dataloader(self, batch_size, shuffle=False, num_workers=0, **kwargs):
        return DataLoader(self, batch_size=batch_size, shuffle=shuffle,
                          num_workers=num_workers, collate_fn=self.collate_fn,
                          **kwargs)


class MaskedFastaTokenDataset(FastaTokenDataset):
    '''
    Dataset for reading fasta files into tokenized sequences with masked tokens
    and a fixed number of tokens per batch.

    Args:
        fasta (str): Path to the fasta file.
        fai (str, optional): Path to the fasta index file. If not provided,
            it is assumed to be `fasta + ".fai"`.
        token_per_batch (int): Maximum number of tokens per batch.
        k_sample (int, optional): Number of sequences to sample from the fasta
            file. If None, all sequences are used.
        max_len (int, optional): Maximum length of the sequences to include.
            If None, all sequences are used.
        mask_freq (float): Frequency of masked tokens.
        alter_freq (float): Frequency of altered tokens and same frequency of
            masked tokens will be replace with original tokens.
        drop_last (bool): If True, drop the last batch if it is smaller than
            `token_per_batch`.
        shuffle (bool): If True, shuffle the sequences.
        random_state (int): Random seed for shuffling the sequences
    '''

    def __init__(self, fasta, fai=None, token_per_batch=50_000, k_sample=None,
                 max_len=None, mask_freq=.15, alter_freq=.1, drop_last=False,
                 shuffle=True, random_state=None, alphabet=Alphabet3):
        super().__init__(fasta, fai=fai, token_per_batch=token_per_batch,
                         k_sample=k_sample, max_len=max_len, drop_last=drop_last,
                         shuffle=shuffle, random_state=random_state, alphabet=alphabet)
        self.mask_freq = mask_freq
        self.alter_freq = alter_freq

    def __getitem__(self, idx):
        tokens, unpad_args = super().__getitem__(idx)
        mtokens, mask = mask_tokens(
            tokens, self.mask_freq, self.alter_freq, alphabet=self.alphabet)
        return tokens, unpad_args, mtokens, mask


class MaskedFastaDataModule(L.LightningDataModule):
    '''
    DataModule for reading fasta files into tokenized sequences with masked tokens.

    Args:
        train_fasta (str): Path to the training fasta file.
        val_fasta (str): Path to the validation fasta file.
        test_fasta (str, optional): Path to the test fasta file.
        train_fai (str, optional): Path to the training fasta index file.
        val_fai (str, optional): Path to the validation fasta index file.
        test_fai (str, optional): Path to the test fasta index file.
        batch_size (int): Batch size.
        num_workers (int): Number of workers for the dataloaders.
        mask_freq (float): Frequency of masked tokens.
        alter_freq (float): Frequency of altered
        max_len (int): Maximum length of the sequences to include.
    '''

    def __init__(
        self, train_fasta, val_fasta, test_fasta=None,
        train_fai=None, val_fai=None, test_fai=None,
        batch_size=16, num_workers=0, mask_freq=.15, alter_freq=.1, max_len=None, alphabet=Alphabet3
    ):
        super().__init__()
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.train_fasta = train_fasta
        self.val_fasta = val_fasta
        self.test_fasta = test_fasta

        self.train_fai = train_fai
        self.val_fai = val_fai
        self.test_fai = test_fai

        self.mask_freq = mask_freq
        self.alter_freq = alter_freq
        self.max_len = max_len
        self.alphabet = alphabet

    def _dataloder(self, fasta, fai=None, shuffle=False):
        return MaskedFastaDataset(
            fasta, fai=fai, max_len=self.max_len, mask_freq=self.mask_freq,
            alter_freq=self.alter_freq, alphabet=self.alphabet
        ).to_dataloader(shuffle=shuffle, batch_size=self.batch_size,
                        num_workers=self.num_workers)

    def train_dataloader(self):
        return self._dataloder(self.train_fasta, self.train_fai, shuffle=True)

    def val_dataloader(self):
        return self._dataloder(self.val_fasta, self.val_fai)

    def test_dataloader(self):
        return self._dataloder(self.test_fasta, self.test_fai)


class MaskedFastaTokenDataModule(L.LightningDataModule):
    '''
    DataModule for reading fasta files into tokenized sequences with masked tokens
    and a fixed number of tokens per batch.

    Args:
        train_fasta (str): Path to the training fasta file.
        val_fasta (str): Path to the validation fasta file.
        test_fasta (str, optional): Path to the test fasta file.
        train_fai (str, optional): Path to the training fasta index file.
        val_fai (str, optional): Path to the validation fasta index file.
        test_fai (str, optional): Path to the test fasta index file.
        token_per_batch (int): Maximum number of tokens per batch.
        num_workers (int): Number of workers for the dataloaders.
        mask_freq (float): Frequency of masked tokens.
        alter_freq (float): Frequency of altered tokens and same frequency of
            masked tokens will be replace with original tokens.
        max_len (int): Maximum length of the sequences to include.
    '''

    def __init__(
        self, train_fasta, val_fasta, test_fasta=None,
        train_fai=None, val_fai=None, test_fai=None,
        token_per_batch=100_000, num_workers=0,
        mask_freq=.15, alter_freq=.1, max_len=None, alphabet=Alphabet3
    ):
        super().__init__()
        self.token_per_batch = token_per_batch
        self.num_workers = num_workers
        self.train_fasta = train_fasta
        self.val_fasta = val_fasta
        self.test_fasta = test_fasta

        self.train_fai = train_fai
        self.val_fai = val_fai
        self.test_fai = test_fai

        self.mask_freq = mask_freq
        self.alter_freq = alter_freq
        self.max_len = max_len

        self.alphabet = alphabet
        self.current_epoch = 0

    def _dataloder(self, fasta, fai=None, shuffle=False):
        return MaskedFastaTokenDataset(
            fasta, fai=fai, token_per_batch=self.token_per_batch, max_len=self.max_len,
            mask_freq=self.mask_freq, alter_freq=self.alter_freq,
            shuffle=shuffle, random_state=self.current_epoch, alphabet=self.alphabet
        ).to_dataloader(num_workers=self.num_workers)

    def train_dataloader(self):
        return self._dataloder(self.train_fasta, self.train_fai, shuffle=True)

    def val_dataloader(self):
        return self._dataloder(self.val_fasta, self.val_fai)

    def test_dataloader(self):
        return self._dataloder(self.test_fasta, self.test_fai)

    def set_epoch(self, epoch):
        self.current_epoch = epoch


class SetEpochCallback(L.pytorch.callbacks.Callback):
    '''
    Callback to set the current epoch in the datamodule. This is useful when
    the datamodule needs to shuffle the sequences at the beginning of each epoch.
    '''

    def on_train_epoch_start(self, trainer, pl_module):
        trainer.datamodule.set_epoch(trainer.current_epoch)
