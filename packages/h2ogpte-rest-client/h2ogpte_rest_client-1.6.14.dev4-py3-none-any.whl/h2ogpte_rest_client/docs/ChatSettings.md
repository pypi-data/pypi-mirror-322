# ChatSettings


## Properties

Name | Type | Description | Notes
------------ | ------------- | ------------- | -------------
**llm** | **str** | LLM name to send the query. Use \&quot;auto\&quot; for automatic model routing, set cost_controls of llm_args for detailed control over automatic routing. | [optional] 
**llm_args** | **Dict[str, object]** | A map of arguments sent to LLM with query.   * &#x60;temperature&#x60; **(type&#x3D;double, default&#x3D;0.0)** - A value used to modulate the next token probabilities.     0 is the most deterministic and 1 is most creative.   * &#x60;top_k&#x60; **(type&#x3D;integer, default&#x3D;1)** - A number of highest probability vocabulary tokens to keep for top-k-filtering.   * &#x60;top_p&#x60; **(type&#x3D;double, default&#x3D;0.0)** - If set to a value &lt; 1, only the smallest set of most probable     tokens with probabilities that add up to top_p or higher are kept for generation.   * &#x60;seed&#x60; **(type&#x3D;integer, default&#x3D;0)** - A seed for the random number generator when sampling during      generation (if temp&gt;0 or top_k&gt;1 or top_p&lt;1), seed&#x3D;0 picks a random seed.   * &#x60;repetition_penalty&#x60; **(type&#x3D;double, default&#x3D;1.07)** - A parameter for repetition penalty. 1.0 means no penalty.   * &#x60;max_new_tokens&#x60; **(type&#x3D;double, default&#x3D;1024)** - A maximum number of new tokens to generate.     This limit applies to each (map+reduce) step during summarization and each (map) step during extraction.   * &#x60;min_max_new_tokens&#x60; **(type&#x3D;integer, default&#x3D;512)** - A minimum value for max_new_tokens when auto-adjusting for content of prompt, docs, etc.   * &#x60;response_format&#x60; **(type&#x3D;enum[text, json_object, json_code], default&#x3D;text)** - An output type of LLM   * &#x60;guided_json&#x60; **(type&#x3D;map)** - If specified, the output will follow the JSON schema.   * &#x60;guided_regex&#x60; **(type&#x3D;string)** - If specified, the output will follow the regex pattern.     Only for models that support guided generation.   * &#x60;guided_choice&#x60; **(type&#x3D;array[string])** - If specified, the output will be exactly one of the choices.     Only for models that support guided generation.   * &#x60;guided_grammar&#x60; **(type&#x3D;string)** - If specified, the output will follow the context free grammar.     Only for models that support guided generation.   * &#x60;guided_whitespace_pattern&#x60; **(type&#x3D;string)** - If specified, will override the default whitespace pattern for guided json decoding.     Only for models that support guided generation.   * &#x60;enable_vision&#x60; **(type&#x3D;enum[on, off, auto], default&#x3D;auto)** - Controls vision mode,     send images to the LLM in addition to text chunks.   * &#x60;visible_vision_models&#x60; **(type&#x3D;array[string], default&#x3D;[auto])** - Controls which vision model to use when processing images.     Must provide exactly one model. [auto] for automatic.   * &#x60;cost_controls&#x60; **(type&#x3D;map)** A map with cost controls settings:     * &#x60;max_cost&#x60; **(type&#x3D;double)** - Sets the maximum allowed cost in USD per LLM call when doing Automatic model routing.       If the estimated cost based on input and output token counts is higher than this limit,       the request will fail as early as possible.     * &#x60;max_cost_per_million_tokens&#x60; **(type&#x3D;double)** - Only consider models that cost less than this value in USD per million tokens       when doing automatic routing. Using the max of input and output cost.     * &#x60;model&#x60; **(type&#x3D;array[string])** - Optional subset of models to consider when doing automatic routing.       If not specified, all models are considered.     * &#x60;willingness_to_pay&#x60; **(type&#x3D;double)** - Controls the willingness to pay extra for a more accurate model for every LLM call       when doing automatic routing, in units of USD per +10% increase in accuracy.       We start with the least accurate model. For each more accurate model,       we accept it if the increase in estimated cost divided by the increase in estimated accuracy       is no more than this value divided by 10%, up to the upper limit specified above.       Lower values will try to keep the cost as low as possible,       higher values will approach the cost limit to increase accuracy. 0 means unlimited.     * &#x60;willingness_to_wait&#x60; **(type&#x3D;double)** - Controls the willingness to wait longer for a more accurate model for every LLM call       when doing automatic routing, in units of seconds per +10% increase in accuracy.       We start with the least accurate model. For each more accurate model,       we accept it if the increase in estimated time divided by the increase in estimated accuracy       is no more than this value divided by 10%. Lower values will try to keep the time       as low as possible, higher values will take longer to increase accuracy. 0 means unlimited.  | [optional] 
**self_reflection_config** | **Dict[str, object]** | A map with self reflection settings:   * &#x60;llm_reflection&#x60; **(type&#x3D;string, example&#x3D;gpt-4-0613)**   * &#x60;prompt_reflection&#x60; **(type&#x3D;string, example&#x3D;\\\&quot;\\\&quot;\\\&quot;Prompt:\\\\\\\\n%s\\\\\\\\n\\\&quot;\\\&quot;\\\&quot;\\\\\\\\n\\\\\\\\n\\\&quot;\\\&quot;\\\&quot;)**   * &#x60;system_prompt_reflection&#x60; **(type&#x3D;string)**   * &#x60;llm_args_reflection&#x60; **(type&#x3D;string, example&#x3D;{})**  | [optional] 
**rag_config** | **object** | A map with arguments to control RAG (retrieval-augmented-generation) types.:   * &#x60;rag_type&#x60; **(type&#x3D;enum[auto, llm_only, rag, hyde1, hyde2, rag+, all_data])** RAG type options:     * &#x60;auto&#x60; - Automatically select the best rag_type.     * &#x60;llm_only&#x60; LLM Only - Answer the query without any supporting document contexts.        Requires 1 LLM call.     * &#x60;rag&#x60; RAG (Retrieval Augmented Generation) - Use supporting document contexts        to answer the query. Requires 1 LLM call.     * &#x60;hyde1&#x60; LLM Only + RAG composite - HyDE RAG (Hypothetical Document Embedding).        Use &#39;LLM Only&#39; response to find relevant contexts from a collection for generating        a response. Requires 2 LLM calls.     * &#x60;hyde2&#x60; HyDE + RAG composite - Use the &#39;HyDE RAG&#39; response to find relevant        contexts from a collection for generating a response. Requires 3 LLM calls.     * &#x60;rag+&#x60; Summary RAG - Like RAG, but uses more context and recursive        summarization to overcome LLM context limits. Keeps all retrieved chunks, puts        them in order, adds neighboring chunks, then uses the summary API to get the        answer. Can require several LLM calls.     * &#x60;all_data&#x60; All Data RAG - Like Summary RAG, but includes all document        chunks. Uses recursive summarization to overcome LLM context limits.        Can require several LLM calls.   * &#x60;hyde_no_rag_llm_prompt_extension&#x60; **(type&#x3D;string, example&#x3D;\\\\\\\\nKeep the answer brief, and list the 5 most relevant key words at the end.)** -     Add this prompt to every user&#39;s prompt, when generating answers to be used for subsequent retrieval during HyDE.     Only used when rag_type is &#x60;hyde1&#x60; or &#x60;hyde2&#x60;.   * &#x60;num_neighbor_chunks_to_include&#x60; **(type&#x3D;integer, default&#x3D;1)** - A number of neighboring chunks to include      for every retrieved relevant chunk. It helps to keep surrounding context together. Only enabled for rag_type &#x60;rag+&#x60;.   * &#x60;meta_data_to_include&#x60; **(type&#x3D;map)** - A map with flags that indicate whether each piece of document metadata      is to be included as part of the context for a chat with a collection.     * &#x60;name&#x60; **(type: boolean, default&#x3D;True)**     * &#x60;text&#x60; **(type: boolean, default&#x3D;True)**     * &#x60;page&#x60; **(type: boolean, default&#x3D;True)**     * &#x60;captions&#x60; **(type: boolean, default&#x3D;True)**     * &#x60;uri&#x60; **(type: boolean, default&#x3D;False)**     * &#x60;connector&#x60; **(type: boolean, default&#x3D;False)**     * &#x60;original_mtime&#x60; **(type: boolean, default&#x3D;False)**     * &#x60;age&#x60; **(type: boolean, default&#x3D;False)**     * &#x60;score&#x60; **(type: boolean, default&#x3D;False)**   * &#x60;rag_max_chunks&#x60; **(type&#x3D;integer, default&#x3D;-1)** - Maximum number of document chunks to retrieve for RAG.     Actual number depends on rag_type and admin configuration. Set to &gt;0 values to enable.      Can be combined with rag_min_chunk_score.   * &#x60;rag_min_chunk_score&#x60; **(type&#x3D;double, default&#x3D;0.0)** - Minimum score of document chunks to retrieve for RAG.     Set to &gt;0 values to enable. Can be combined with rag_max_chunks.  | [optional] 
**include_chat_history** | **str** | Whether to include chat history. Includes previous questions and answers for the current chat session for each new chat request. Disable if require deterministic answers for a given question. | [optional] 
**tags** | **List[str]** | A list of tags from which to pull the context for RAG. | [optional] 

## Example

```python
from h2ogpte_rest_client.models.chat_settings import ChatSettings

# TODO update the JSON string below
json = "{}"
# create an instance of ChatSettings from a JSON string
chat_settings_instance = ChatSettings.from_json(json)
# print the JSON string representation of the object
print(ChatSettings.to_json())

# convert the object into a dict
chat_settings_dict = chat_settings_instance.to_dict()
# create an instance of ChatSettings from a dict
chat_settings_from_dict = ChatSettings.from_dict(chat_settings_dict)
```
[[Back to Model list]](../README.md#documentation-for-models) [[Back to API list]](../README.md#documentation-for-api-endpoints) [[Back to README]](../README.md)


