Metadata-Version: 2.1
Name: pyperustats
Version: 0.1.2
Summary: Allows downloading data from various data sources in Peru.
Home-page: https://github.com/TJhon/PyPeruStats
License: Apache-2.0
Author: Jhon K. Flores Rojas
Author-email: fr.jhonk@gmail.com
Requires-Python: >=3.10,<4.0
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Requires-Dist: bs4 (>=0.0.2,<0.0.3)
Requires-Dist: fastparquet (>=2024.11.0,<2025.0.0)
Requires-Dist: ipykernel (>=6.29.5,<7.0.0)
Requires-Dist: lxml (>=5.3.0,<6.0.0)
Requires-Dist: numpy (>=2.2.0,<3.0.0)
Requires-Dist: pandas (>=2.2.3,<3.0.0)
Requires-Dist: pyarrow (>=18.1.0,<19.0.0)
Requires-Dist: requests (>=2.32.3,<3.0.0)
Requires-Dist: termcolor (>=2.5.0,<3.0.0)
Requires-Dist: tqdm (>=4.67.1,<5.0.0)
Requires-Dist: unidecode (>=1.3.8,<2.0.0)
Description-Content-Type: text/markdown

# PyPeruStats

Allows downloading data from various data sources in Peru.

Sources: INEI, BCRP

### Installation

```bash
pip install pyperustats
```



## INEI

### Parameters Description

#### MICRODATOS_INEI

- `survey`: Survey type ('enaho', 'enapres', 'endes')
  - Available up to 2024-Quarter 3

#### download_default

- `format`: Output file format
  - 'csv': CSV files
  - 'stata': Stata files
  - 'spss': SPSS files
- `force`: Force re-download of existing files
- `remove_zip`: Remove ZIP files after extraction
- `workers`: Number of workers for parallel download
- `zip_dir`: Directory to store ZIP files

#### organize_files

- `dir_output`: Directory where organized files will be saved
- `order_by`: Organization method
  - 'modules': Structure "mod_01/year_n.csv"
- `ext_documentation`: List of documentation extensions
- `delete_master_dir`: Delete master directory after organizing

### USAGE

```py
from pyPeruStats import MICRODATOS_INEI, print_tree

# Options: enaho, enapres, endes, available up to 2024-Quarter 3
enaho = MICRODATOS_INEI(survey="enaho") 
modules = enaho.modules
# Found modules 
print(modules.head(2))
```

```
   codigo_modulo                                      modulo      anio
0              1  CaracterÃ­sticas de la Vivienda y del Hogar  2024 ...
1              2   CaracterÃ­sticas de los Miembros del Hogar  2024 ...
```

```py
downloaded = enaho.search(
    [2021, 2023, 2004, 2006, 2007, 2008], [1, 2, 3, 8]
).download_default(
    format='csv', # csv, stata, spss
    force=False, # download zip files again
    remove_zip=False, # remove original zips from microdata page
    workers=4,  # Parallel download
    zip_dir="trash_zips" # where zips will be downloaded
)

# Downloaded files within directory
print_tree('./trash_zips/')
```

```
ðŸ“ trash_zips
â””â”€â”€ ðŸ“ inei_enaho_download
    â”œâ”€â”€ ðŸ“ 2004
        â”œâ”€â”€ ðŸ“ 2004_01
        â”‚   â””â”€â”€ ðŸ“ 280-Modulo01
        â”‚   â”‚   â”œâ”€â”€ ðŸ“„ CED-01-100 2004.pdf
        â”‚   â”‚   â”œâ”€â”€ ðŸ“„ Diccionario.pdf
        â”‚   â”‚   â”œâ”€â”€ ðŸ“„ enaho01-2004-100.dta
        â”‚   â”‚   â””â”€â”€ ðŸ“„ Ficha Tecnica - 2004.pdf
        â”œâ”€â”€ ðŸ“ 2004_02
....
```

```py
result_files = downloaded.organize_files(
    dir_output="./data_inei/", # Where files will be saved
    order_by="modules", # modules: file structure "mod_01/year_n.csv" ; # year: file structure year_n/mod_n
    ext_documentation=['pdf'], # files used for documentation
    delete_master_dir=False # true if you want to delete all zip files and unzip again (use with caution)
)
print_tree("./data_inei/") # print file structure
```

```
ðŸ“ data_inei
â”œâ”€â”€ ðŸ“ documentation_pdf
    â”œâ”€â”€ ðŸ“„ 2004_01_ced-01-100_2004.pdf
    â”œâ”€â”€ ðŸ“„ 2004_01_diccionario.pdf
    â”œâ”€â”€ ðŸ“„ 2004_01_ficha_tecnica_-_2004
...
â””â”€â”€ ðŸ“ modules
    â”œâ”€â”€ ðŸ“ 001
        â”œâ”€â”€ ðŸ“„ 2004.dta
        â”œâ”€â”€ ðŸ“„ 2006.dta
        â”œâ”€â”€ ðŸ“„ 2007.dta
        â”œâ”€â”€ ðŸ“„ 2008.csv
        â”œâ”€â”€ ðŸ“„ 2021.csv
        â””â”€â”€ ðŸ“„ 2023.csv
    â”œâ”€â”€ ðŸ“ 002
        â”œâ”€â”€ ðŸ“„ 2004.dta
        â”œâ”€â”€ ðŸ“„ 2006.dta
        â”œâ”€â”€ ðŸ“„ 2007.dta
        â”œâ”€â”€ ðŸ“„ 2008.csv
....
```

### Notes

1. Parallel download significantly improves performance but consumes more resources
2. It's recommended to keep original ZIP files as backup
3. Check disk space before downloading multiple years/modules
4. Documentation files are organized in a separate directory


## BCRP

### Current Issues with the Source Data

1. Inconsistent Data Formats Across Frequencies
   - **Spanish Month Abbreviations**  
     For example: `"Ene05"` (January 2005 in Spanish format).  
   - **Complex Date Strings**  
     Example: `"31Ene05"` combines day, month (abbreviated in Spanish), and year, requiring parsing.  
   - **Quarterly Indicators**  
     Example: `"T113"` indicates the 1st quarter of 2013 and needs transformation to a standard format.  

2. Additional Steps Required for Proper DataFrame Conversion
   - Converting non-standard date strings to a format recognized by `pandas` or similar libraries.  
   - Harmonizing date formats across daily, monthly, quarterly, and annual frequencies.  

3. Slow Response Time from the BCRP UI
   - The platform often experiences delays when fetching data, impacting the efficiency of workflows.  


### Features

- Seamless data retrieval across different time frequencies
- Automatic conversion of Spanish date formats to standard datetime
- Parallel processing capabilities
- Built-in caching mechanism
- Flexible data processing



```py
from pyPeruStats import BCRPDataProcessor

# Define series codes
diarios = ["PD38032DD", "PD04699XD"]
mensuales = ["RD38085BM", "RD38307BM"]
trimestrales = ["PD37940PQ", "PN38975BQ"]
anuales = [
    "PM06069MA",
    "PM06078MA",
    "PM06101MA",
    "	PM06088MA",
    "PM06087MA",
    "	PM06086MA",
    "	PM06085MA",
    "	PM06084MA",
    "	PM06083MA",
    "	PM06082MA",
    "	PM06081MA",
    "	PM06070MA",
]

# Combine all frequencies
all_freq = diarios + mensuales + trimestrales + anuales

# Initialize processor
processor = BCRPDataProcessor(
    all_freq, 
    start_date="2002-01-02", 
    end_date="2023-01-01", 
    parallel=True
)

# Process data
data = processor.process_data(save_sqlite=True)

# Access DataFrames by frequency
anuales_df = data.get("A")
trimestrales_df = data.get("Q")
mensuales_df = data.get("M")
diarios_df = data.get("D")
```



### Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

### License

Apache 2.0

### Contact

fr.jhonk@gmail.com

# TODO

- BCRP
  - [x] Download statistical data from BCRP
  - [ ] Implement advanced data search functionality
  - [ ] Create autoplot functionality (inspired by ggplot)
  - [ ] Set up GitHub repository and backup mechanism
  - [ ] Add comprehensive documentation
  - [ ] Create example notebooks
