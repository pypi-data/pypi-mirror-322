# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/core/criteria.ipynb.

# %% auto 0
__all__ = ['random', 'large_final', 'squared_final', 'small_final', 'large_init', 'small_init', 'large_init_large_final',
           'small_init_small_final', 'magnitude_increase', 'movement', 'updating_magnitude_increase',
           'updating_movement', 'movmag', 'updating_movmag', 'criterias', 'Criteria', 'available_criterias',
           'grad_crit']

# %% ../../nbs/core/criteria.ipynb 2
import torch
import torch.nn as nn
import torch.nn.functional as F
from fastcore.basics import *
from fastcore.imports import *
from .granularity import *
from typing import Callable

# %% ../../nbs/core/criteria.ipynb 6
class Criteria():
    def __init__(self, f:Callable, reducer:str='mean', normalizer:str=None, needs_init:bool=False, needs_update:bool=False, output_f:Callable=None, return_init=False):
        store_attr()
        assert (needs_init and needs_update)==False, "The init values will be overwritten by the updating ones."
   
    @torch.no_grad()
    def __call__(self, m, g, squeeze=False):
        try:
            dim = listify(Granularities.get_dim(m, g))
        except KeyError:
            raise NotImplementedError('Invalid granularity')
            
        if self.needs_update and hasattr(m, '_old_weights') == False:
            m.register_buffer("_old_weights", m._init_weights.clone()) # If the previous value of weights is not known, take the initial value
            
        wf = self.f(m.weight)
        
        if self.needs_init: wi = self.f(m._init_weights)
        if self.needs_update: wi = self.f(m._old_weights)
        
        if self.output_f: scores = self.output_f(wf, wi)
        elif self.return_init: scores = wi
        else: scores = wf
            
        scores = self._rescale(scores)
        if hasattr(m, '_mask'): scores.mul_(m._mask)
        scores = self._reduce(scores, dim)
        scores = self._normalize(scores)
        if squeeze: scores = scores[None].squeeze((0,*dim))
        return scores
    
    def _reduce(self, scores, dim):
        if self.reducer == "sum":
            return scores[None].sum(dim=dim, keepdim=True).squeeze(0)
        elif self.reducer == "mean":
            return scores[None].mean(dim=dim, keepdim=True).squeeze(0)
        else: 
            raise NotImplementedError('Invalid reducer')
            
    def _normalize(self, scores):
        if self.normalizer is None: 
            return scores
        elif isinstance(self.normalizer, Callable):
            return self.normalizer(scores)
        elif self.normalizer == "sum":
            return scores / scores.sum()
        elif self.normalizer == "standardization":
            return (scores - scores.min()) / (scores.max() - scores.min()+torch.finfo(torch.float32).eps)
        elif self.normalizer == "mean":
            return scores / scores.mean()
        elif self.normalizer == "max":
            return scores / scores.max()
        elif self.normalizer == 'gaussian':
            return (scores - scores.mean()) / (scores.std()+torch.finfo(torch.float32).eps)
        else:
            raise NotImplementedError('Invalid normalizer')

    def _rescale(self, scores): # Rescale scores to be >0, thus avoiding not pruning previously pruned weight (with a value of 0)
        scores += scores.min().abs() + torch.finfo(torch.float32).eps
        return scores

    def update_weights(self, m):
        if self.needs_update: 
            m._old_weights = m.weight.data.clone() # The current value becomes the old one for the next iteration

# %% ../../nbs/core/criteria.ipynb 9
random = Criteria(torch.randn_like)

# %% ../../nbs/core/criteria.ipynb 12
large_final = Criteria(torch.abs)

# %% ../../nbs/core/criteria.ipynb 15
squared_final = Criteria(torch.square)

# %% ../../nbs/core/criteria.ipynb 18
small_final = Criteria(compose(torch.abs, torch.neg))

# %% ../../nbs/core/criteria.ipynb 21
large_init = Criteria(torch.abs, needs_init=True, return_init=True)

# %% ../../nbs/core/criteria.ipynb 24
small_init = Criteria(compose(torch.abs, torch.neg), needs_init=True, return_init=True)

# %% ../../nbs/core/criteria.ipynb 27
large_init_large_final = Criteria(torch.abs, needs_init=True, output_f=torch.min)

# %% ../../nbs/core/criteria.ipynb 30
small_init_small_final = Criteria(torch.abs, needs_init=True, output_f=lambda x,y: torch.neg(torch.max(x,y)))

# %% ../../nbs/core/criteria.ipynb 33
magnitude_increase = Criteria(torch.abs, needs_init=True, output_f= torch.sub)

# %% ../../nbs/core/criteria.ipynb 36
movement = Criteria(noop, needs_init=True, output_f= lambda x,y: torch.abs(torch.sub(x,y)))

# %% ../../nbs/core/criteria.ipynb 41
updating_magnitude_increase = Criteria(torch.abs, needs_update=True, output_f= lambda x,y: torch.sub(x,y))

# %% ../../nbs/core/criteria.ipynb 44
updating_movement = Criteria(noop, needs_update=True, output_f= lambda x,y: torch.abs(torch.sub(x,y)))

# %% ../../nbs/core/criteria.ipynb 47
movmag = Criteria(noop, needs_init=True, output_f=lambda x,y: torch.abs(torch.mul(x, torch.sub(x,y))))

# %% ../../nbs/core/criteria.ipynb 50
updating_movmag = Criteria(noop, needs_update=True, output_f=lambda x,y: torch.abs(torch.mul(x, torch.sub(x,y))))

# %% ../../nbs/core/criteria.ipynb 52
criterias = ('random', 'large_final', 'small_final', 'squared_final', 'small_init', 'small_final', 'large_init_large_final', 'small_init_small_final', 'magnitude_increase', 'movement', 'updating_magnitude_increase', 'updating_movement', 'updating_movmag')
def available_criterias():
    print(criterias)

# %% ../../nbs/core/criteria.ipynb 73
def grad_crit(m, g):
    if g in granularities[m.__class__.__name__]: 
        dim = granularities[m.__class__.__name__][g]
        if m.weight.grad is not None:
            return (m.weight*m.weight.grad)[None].pow(2).mean(dim=dim, keepdim=True).squeeze(0)
        else: 
            return m.weight[None].pow(2).mean(dim=dim, keepdim=True).squeeze(0)
    else: raise NameError('Invalid Granularity') 
