# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/sparse/sparsifier.ipynb.

# %% auto 0
__all__ = ['Sparsifier']

# %% ../../nbs/sparse/sparsifier.ipynb 3
import numpy as np
import torch
import torch.nn as nn
import pickle
from itertools import cycle
from fastcore.basics import store_attr, listify, true
from ..core.criteria import *

# %% ../../nbs/sparse/sparsifier.ipynb 5
class Sparsifier():
    "Class providing sparsifying capabilities"
    def __init__(self, model, granularity, context, criteria, nm=False, layer_type=nn.Conv2d):
        if nm == True: print('Sparsity automatically set to 50%')
        store_attr()
        self._save_weights() # Save the original weights
        self._reset_threshold()

    def sparsify_layer(self, m, sparsity, round_to=None):
        scores    = self._compute_scores(m, sparsity)
        threshold = self._compute_threshold(scores, sparsity, round_to)
        mask      = self._compute_mask(scores, threshold)
        m.register_buffer('_mask', mask)
        self._apply(m)
        self.criteria.update_weights(m)

    def sparsify_model(self, sparsity, round_to=None):
        self._reset_threshold()
        sparsity_list = listify(sparsity)
        if len(sparsity_list)>1: assert self.context=='local', f"A list of sparsities cannot be passed using: {self.context}"
        sparsities = cycle(sparsity_list) if len(sparsity_list)==1 else iter(sparsity_list)
        mods = list(self.model.modules())
        for k,m in enumerate(self.model.modules()):
            if isinstance(m, self.layer_type): 
                sp = next(sparsities)
                self.sparsify_layer(m, sp, round_to)
                if isinstance(mods[k+1], nn.modules.batchnorm._BatchNorm): self.sparsify_batchnorm(m, mods[k+1])
                
    def sparsify_batchnorm(self, m, bn):
        mask = getattr(m, "_mask", None)
        if self.granularity == 'filter' and true(mask):
            bn.weight.data.mul_(mask.squeeze())
            bn.bias.data.mul_(mask.squeeze())
            
    def _apply_masks(self):
        for m in self.model.modules():
            if isinstance(m, self.layer_type):
                self._apply(m)
        
    def _apply(self, m):
        mask = getattr(m, "_mask", None)
        if true(mask): m.weight.data.mul_(mask)
        if self.granularity == 'filter' and true(m.bias):
            if true(mask): m.bias.data.mul_(mask.squeeze()) # We want to sparsify the bias when pruning filters
    
    def _reset_weights(self, model=None):
        model = model or self.model
        for m in model.modules():
            if hasattr(m, 'weight'):
                init_weights = getattr(m, "_init_weights", m.weight)
                init_biases = getattr(m, "_init_biases", m.bias)
                with torch.no_grad():
                    if true(m.weight): m.weight.copy_(init_weights)
                    if true(m.bias): m.bias.copy_(init_biases)
                self._apply(m)
            if isinstance(m, nn.modules.batchnorm._BatchNorm): m.reset_parameters()
                
    def _save_weights(self):
        for m in self.model.modules():
            if hasattr(m, 'weight'):              
                m.register_buffer("_init_weights", m.weight.clone())
                bias = getattr(m, 'bias', None)
                if true(bias): m.register_buffer("_init_biases", bias.clone())
                    
    def save_model(self, path, model=None):
        model = model or self.model
        tmp_model = pickle.loads(pickle.dumps(model))
        self._reset_weights(tmp_model)
        self._clean_buffers(tmp_model)
        torch.save(tmp_model, path)

    def _clean_buffers(self, model=None):
        model = model or self.model
        for m in model.modules():
            if hasattr(m, 'weight'):
                if hasattr(m, '_mask'): del m._buffers["_mask"]
                if hasattr(m, '_init_weights'): del m._buffers["_init_weights"]
                if hasattr(m, '_init_biases'): del m._buffers["_init_biases"]
                    
    def _reset_threshold(self):
        self.threshold=None
            
    def _rounded_sparsity(self, n_to_prune, round_to):
        return max(round_to*torch.ceil(n_to_prune/round_to), round_to)
    
    def _compute_scores(self, m, sparsity):
        if self.context == 'global':
            if self.threshold == None: 
                global_scores  = torch.cat([self.criteria(m, self.granularity).view(-1) for m in self.model.modules() if isinstance(m, self.layer_type)])
                self.threshold = torch.quantile(global_scores.view(-1), sparsity/100)
            local_scores = self.criteria(m, self.granularity)
        elif self.context == 'local': 
            local_scores = self.criteria(m, self.granularity)
            self.threshold = torch.quantile(local_scores.view(-1), sparsity/100)
        else: raise NameError('Invalid Context')
        return local_scores
                
    def _compute_threshold(self, scores, sparsity, round_to):
        if round_to:
            n_to_keep = sum(scores.ge(self.threshold)).squeeze()
            self.threshold = torch.topk(scores.squeeze(), int(self._rounded_sparsity(n_to_keep, round_to)))[0].min()
        return self.threshold
    
    def _compute_mask(self, scores, threshold):
        if self.nm == True: return self.apply_nm_sparsity(scores)
        if threshold > scores.max(): threshold = scores.max() # Make sure we don't remove every weight of a given layer
        return scores.ge(threshold).to(dtype=scores.dtype)
    
    def print_sparsity(self):
        for k,m in enumerate(self.model.modules()):
            if isinstance(m, self.layer_type):
                print(f"Sparsity in {m.__class__.__name__} {k}: {100. * float(torch.sum(m.weight == 0))/ float(m.weight.nelement()):.2f}%")

    def apply_nm_sparsity(self, scores):
        out_channels, in_channels, kernel_height, kernel_width = scores.shape
        sparse_mask = torch.ones_like(scores)
        if in_channels * kernel_height * kernel_width % 16 != 0:
            print(f"Skipping 2:4 sparsity, Cin * Kh * Kw is not a multiple of 16")
            return sparse_mask  # Return weights unchanged if condition is not met
        for out_ch in range(out_channels):
            for h in range(kernel_height):
                for w in range(kernel_width):
                    kernel_weights = scores[out_ch, :, h, w]
                    blocks = kernel_weights.view(-1, 4)  # Flatten into blocks of 4
                    _, indices = blocks.topk(2, dim=1, largest=True, sorted=False)  # Retain top-2 absolute values in each block
                    mask = torch.zeros_like(blocks)
                    mask.scatter_(1, indices, 1)
                    sparse_mask[out_ch, :, h, w] = mask.view(-1)  # Reshape and place the mask in the appropriate location
        return sparse_mask
