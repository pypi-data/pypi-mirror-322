Metadata-Version: 2.2
Name: streamlit-ollama-agent
Version: 0.1.1
Summary: A reusable Streamlit-based chat interface for Ollama models using PydanticAI
Home-page: https://github.com/yourusername/streamlit-ollama-agent
Author: Shenakiii
Author-email: re@dac.ted
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: streamlit>=1.24.0
Requires-Dist: openai>=1.0.0
Requires-Dist: pydantic-ai>=0.0.1
Requires-Dist: httpx>=0.24.0
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# Streamlit Ollama Agent

A reusable Python package that provides a Streamlit-based chat interface for Ollama models using PydanticAI. This package makes it easy to create chat applications with streaming responses and conversation history.

## Features

- ğŸš€ Easy-to-use Streamlit chat interface
- ğŸ“ Streaming responses with real-time updates
- ğŸ’¬ Conversation history support
- ğŸ”„ Compatible with any Ollama model
- ğŸ›  Built on PydanticAI for robust type safety
- âš¡ Dual streaming modes: OpenAI compatibility or direct Ollama

## Installation

```bash
# From PyPI (once published)
pip install streamlit-ollama-agent

```

## Quick Start

1. Make sure you have Ollama running locally:
```bash
ollama run llama2  # or your preferred model
```

2. Create a new Python file (e.g., `chat_app.py`):
```python
import streamlit as st
from streamlit_ollama_agent import OllamaAgent

# Initialize the agent (using OpenAI compatibility mode)
agent = OllamaAgent(
    model_name="llama2",  # or your preferred model
    base_url="http://localhost:11434/v1"
)

# Or use direct Ollama streaming
agent = OllamaAgent(
    model_name="llama2",
    use_direct_streaming=True  # Bypass OpenAI compatibility for direct Ollama streaming
)

# Create your chat interface
st.title("Chat with Ollama")

# Use the built-in Streamlit app
from streamlit_ollama_agent import create_chat_app
create_chat_app(agent)
```

3. Run your app:
```bash
streamlit run chat_app.py
```

## Advanced Usage

### Streaming Modes

The package supports two streaming modes:

1. **OpenAI Compatibility Mode** (default):
   - Uses OpenAI's API format
   - Compatible with other OpenAI-like APIs
   - More standardized approach
```python
agent = OllamaAgent(
    model_name="llama2",
    base_url="http://localhost:11434/v1",  # Ollama's OpenAI-compatible endpoint
    api_key="ollama"  # Not required for Ollama
)
```

2. **Direct Ollama Streaming**:
   - Connects directly to Ollama's native API
   - Potentially faster
   - Access to Ollama-specific features
```python
agent = OllamaAgent(
    model_name="llama2",
    use_direct_streaming=True
)
```

### Custom Chat Interface

You can create your own custom chat interface using the `OllamaAgent` directly:

```python
import asyncio
from streamlit_ollama_agent import OllamaAgent

agent = OllamaAgent()

async def get_response(prompt, history=None):
    async for chunk in agent.stream_response(prompt, history):
        # Handle each chunk of the response
        yield chunk
```

### Configuration Options

```python
agent = OllamaAgent(
    model_name="your-model",      # Choose your Ollama model
    base_url="your-url",          # Custom Ollama API URL (OpenAI mode)
    api_key="your-key",           # If needed (OpenAI mode)
    use_direct_streaming=False    # Choose streaming mode
)
```

## Project Structure

```
streamlit-ollama-agent/
â”œâ”€â”€ streamlit_ollama_agent/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ agent.py          # OllamaAgent implementation
â”‚   â””â”€â”€ app.py           # Streamlit app implementation
â”œâ”€â”€ setup.py
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the LICENSE file for details. 
