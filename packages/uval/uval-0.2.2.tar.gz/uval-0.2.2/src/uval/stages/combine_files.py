# -*- coding: utf-8 -*-
"""A StageData object acts like a data container that manages results generated by stages and read by subsequent
stages.
"""
import threading
from queue import Queue
from typing import Set

from rich.progress import Progress

from uval.stages.stage import uval_stage  # type: ignore
from uval.stages.stage_data import StageData, SupportedDatasetSpecificationData  # type: ignore
from uval.utils.hdf5_io import UvalHdfFileInput
from uval.utils.log import logger  # type: ignore


def format_fixer(label_data: dict) -> dict:
    if "subclass_name" in label_data:
        label_data["subclass_name"] = (
            label_data["subclass_name"].replace("/", "_").replace(":", "_").replace(" ", "_").replace("-", "_")
        )
    return label_data


def _read_single_file_gt(vol_id, file_path: str, labels):
    with UvalHdfFileInput(file_path) as f:
        gts = f.ground_truth(include_masks=False, include_caches=False)
        label_names = [ll[0] for ll in labels]
        ground_truth = {key: format_fixer(value) for key, value in gts.items() if key in label_names}
        for label_name, label_class in labels:
            if label_name in ground_truth:
                if (gt_cls := ground_truth[label_name].get("class_name", label_class)) != label_class:
                    # if class name exists and mismatches the yaml
                    logger.warning(
                        f"{file_path} : datasplit/data mismatch: labels {label_name}={label_class} is {gt_cls} in GT"
                    )
                ground_truth[label_name]["class_name"] = label_class
        if wrong_labels := set(label_names) - set(ground_truth.keys()):
            logger.warning(f"datasplit and data mismatch: labels {wrong_labels} not found in {file_path}")
        return [StageData(volume_id=vol_id, label_name=lab, **gt) for lab, gt in ground_truth.items()]


def _read_single_file_det(vol_id, file_path: str, class_mappings, minimum_score=0.0, score_name="score"):
    with UvalHdfFileInput(file_path, score_name=score_name) as f:
        det = f.detections(include_masks=False, include_caches=False)
        v1 = len(det)
        det = [el for el in det if el["score"] >= minimum_score]
        if len(det) < v1:
            logger.debug(f"originally the det file had {v1} detections only {len(det)} of the minimum threshold")

        def f(d):
            if d["class_name"]:
                d["class_name"] = class_mappings.get(d["class_name"], d["class_name"])
            else:
                logger.warning(f"no class name in {file_path}")
            return d

        return [StageData(volume_id=vol_id, label_name=None, **f(d)) for d in det]


@uval_stage
def support_dataset_with_file_paths(
    gts: Set,
    files_det: Set,
    files_soc: Set,
    class_mappings={},
    minimum_score=0.0,
    score_name="score",
    scan_level_2d=False,
):
    """Given a dataset and sets of Hdf5 files found on disk, try to combine everything to
    create a supported dataset."""

    files_gt, labels_gt = gts
    gt_ids = set(files_gt.keys())
    det_ids = set(files_det.keys())
    soc_ids = set(files_soc.keys())
    set_union = gt_ids | det_ids | soc_ids
    set_intersection = gt_ids & det_ids
    total_task_count = 2 * (len(set_intersection) + len(soc_ids))
    set_gt_minus_det = gt_ids - det_ids
    set_det_minus_gt = det_ids - gt_ids
    result = SupportedDatasetSpecificationData()
    if len(set_union) == 0:
        logger.error("Did not find any matching files between groundtruth and detections")
        return result
    logger.info(
        f"Dataset Statistics:\n"
        f"GT & DET = {len(set_intersection)}\n"
        f"GT - DET = {len(set_gt_minus_det)}\n"
        f"DET - GT = {len(set_det_minus_gt)}\n"
        f"Negative = {len(soc_ids)}\n"
        f"Positive + Negative = {len(set_union)}\n"
    )

    if len(set_gt_minus_det):
        logger.warning("these ground truth files have no det file associated with them:%s", str(set_gt_minus_det))

    def stem(e):
        return e.split("_v")[0]

    if scan_level_2d:
        black_list_volumes = {stem(el) for el in set_gt_minus_det}
        set_intersection = {el for el in set_intersection if stem(el) not in black_list_volumes}

    data_queue = Queue()
    task_queue = Queue()

    t0 = threading.Thread(
        target=data_producer,
        args=(
            data_queue,
            result,
            files_det,
            files_gt,
            files_soc,
            labels_gt,
            set_intersection,
            soc_ids,
            class_mappings,
            minimum_score,
            score_name,
        ),
    )

    t1 = threading.Thread(target=data_consumer, args=(data_queue, task_queue))
    t2 = threading.Thread(target=task_consumer, args=(task_queue, total_task_count))

    t0.start()
    t1.start()
    t2.start()

    t0.join()
    t1.join()
    t2.join()
    return result


def data_producer(
    data_queue,
    result,
    files_det,
    files_gt,
    files_soc,
    labels_gt,
    set_intersection,
    soc_ids,
    class_mappings={},
    minimum_score=0.0,
    score_name="score",
):
    for vol_id in set_intersection:
        # print(vol_id, files_det[vol_id])
        data_queue.put(
            (
                result.add_detection,
                _read_single_file_det,
                (vol_id, files_det[vol_id], class_mappings, minimum_score, score_name),
            )
        )
        data_queue.put((result.add_ground_truth, _read_single_file_gt, (vol_id, files_gt[vol_id], labels_gt[vol_id])))
    for vol_id in soc_ids:
        data_queue.put(
            (
                result.add_detection,
                _read_single_file_det,
                (vol_id, files_soc[vol_id], class_mappings, minimum_score, score_name),
            )
        )
        data_queue.put((result.add_negative, lambda x: x, (vol_id,)))

    data_queue.put((None, None, None))


def data_consumer(data_queue, task_queue):
    while True:
        ff, f, args = data_queue.get()
        if ff is None:
            task_queue.put((None, None))
            return
        # Submit all jobs to the pool
        task_queue.put((ff, f(*args)))


def task_consumer(task_queue, total_task_count):
    with Progress() as progress:
        task = progress.add_task("[red]Loading...", total=total_task_count)
        while True:
            ff, f = task_queue.get()

            if not ff:
                return

            ff(f)
            progress.update(task, advance=1)
