# coding: utf-8
r"""
ç½‘ç»œçˆ¬è™«Webcrawler
        weatherForecast - å¤©æ°”é¢„æŠ¥
        searchPoetry - æœç´¢è¯—è¯
        todayPoetry - è·å–ä»Šæ—¥è¯—è¯
"""
__version__ = "0.1"
__author__ = "mingwe <shannonhacker@outlook.com>"
# __all__ = [
#     name
#     for name, obj in globals().items()
#     if all(
#         [
#             not name.startswith("_"),
#             not isinstance(obj, types.ModuleType),
#             name not in {"wantobjects"},
#         ]
#     )
# ]


import types
import datetime
import time
import re
import pyperclip
import requests
import json
import os
import sys
from bs4 import BeautifulSoup


def priceCalculate():
    """
    è·å–å‘½ä»¤è¡Œå‚æ•°æˆ–ç”¨æˆ·è¾“å…¥çš„.mdæ–‡ä»¶è·¯å¾„,è§£æè¯¥.mdæ–‡ä»¶ä¸­çš„ä»·æ ¼ä¿¡æ¯,è®¡ç®—æ€»ä»·æ ¼,å¹¶å°†æ€»ä»·æ ¼å¤åˆ¶åˆ°å‰ªè´´æ¿ã€‚
    print: æ‰“å°
    """
    # import sys  # å¯¼å…¥ç³»ç»Ÿæ¨¡å—
    # import os  # å¯¼å…¥OSæ¨¡å—
    # import re
    # import pyperclip
    # from sqlalchemy import false

    def getCMDargv():  # è·å–å‘½ä»¤è¡Œå‚æ•°

        cmd_argv = sys.argv  # è·å–å‘½ä»¤è¡Œå‚æ•°                    #å­˜å‚¨å‘½ä»¤è¡Œå‚æ•°
        py_executable = sys.executable  # å­˜å‚¨Pythonè§£é‡Šå™¨è·¯å¾„

        if sys.stdin.isatty():  # åˆ¤æ–­æ˜¯å¦åœ¨ç»ˆç«¯è¿è¡Œ
            # å¦‚æœåœ¨ç»ˆç«¯è¿è¡Œ
            if len(cmd_argv) > 1:  # å¦‚æœå‘½ä»¤è¡Œå‚æ•°å¤§äº1ä¸ª
                path = cmd_argv[1]  # å–ç¬¬ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ä½œä¸º.mdæ–‡ä»¶è·¯å¾„
        else:
            # å¦‚æœåœ¨.pyæ–‡ä»¶æˆ–Notebookä¸­è¿è¡Œ
            # print('Running in .py file or Notebook')
            path = None
        return path  # è¿”å›.mdæ–‡ä»¶è·¯å¾„

    def diyPC(markdownPath):  # è§£æ.mdæ–‡ä»¶ä¸­çš„ä»·æ ¼ä¿¡æ¯å¹¶è®¡ç®—æ€»ä»·æ ¼
        try:  # å°è¯•æ‰“å¼€.mdæ–‡ä»¶å¹¶è§£æ
            isExists = os.path.exists(markdownPath)  # åˆ¤æ–­æ–‡ä»¶è·¯å¾„æ˜¯å¦å­˜åœ¨
            isFile = os.path.isfile(markdownPath)  # åˆ¤æ–­æ–‡ä»¶è·¯å¾„æ˜¯å¦ä¸ºæ–‡ä»¶
        except Exception as e:  # æ•è·æ‰€æœ‰å¼‚å¸¸ï¼Œå¹¶ä¸”æŠŠé”™è¯¯ä¿¡æ¯ç»™e
            # print(e)
            while True:
                markdownPath = input(
                    "Please input Markdown file path.\n"
                )  # æç¤ºè¾“å…¥æ–‡ä»¶è·¯å¾„
                isExists = os.path.exists(markdownPath)  # åˆ¤æ–­æ–‡ä»¶è·¯å¾„æ˜¯å¦å­˜åœ¨
                isFile = os.path.isfile(markdownPath)  # åˆ¤æ–­æ–‡ä»¶è·¯å¾„æ˜¯å¦ä¸ºæ–‡ä»¶

                if isExists and isFile:  # å¦‚æœè·¯å¾„å­˜åœ¨å¹¶ä¸”æ˜¯æ–‡ä»¶
                    break

        with open(markdownPath, "r", encoding="utf-8") as f:  # æ‰“å¼€.mdæ–‡ä»¶
            mdrows = f.readlines()  # è¯»å–.mdæ–‡ä»¶æ‰€æœ‰è¡Œ

        prices = []  # åˆå§‹åŒ–ä»·æ ¼åˆ—è¡¨
        for row in mdrows:  # éå†.mdæ–‡ä»¶æ¯è¡Œ
            exculde = re.findall(r"\u603b\u4ef7", row)  # æŸ¥æ‰¾æ¯è¡Œæ˜¯å¦åŒ…å«"æ€»ä»·"
            result = re.findall(r"(?<=\Â¥)[\d]+", row)  # æŸ¥æ‰¾æ¯è¡Œä»·æ ¼ä¿¡æ¯
            # print(exculde,result)
            if bool(exculde) == True and exculde[0] == "\u603b\u4ef7":
                continue  # å¦‚æœåŒ…å«"æ€»ä»·",è·³è¿‡å½“å‰è¡Œ

            if bool(result) == False:
                continue  # å¦‚æœå½“å‰è¡Œä¸åŒ…å«ä»·æ ¼,è·³è¿‡å½“å‰è¡Œ
            prices.append(result)  # å°†å½“å‰è¡Œä»·æ ¼æ·»åŠ åˆ° prices åˆ—è¡¨

        # print(prices)
        totalPrice = sum(
            [int(p[0], 10) for p in prices]
        )  # æ±‡æ€»pricesåˆ—è¡¨ä¸­çš„æ‰€æœ‰ä»·æ ¼,è®¡ç®—æ€»ä»·æ ¼

        return totalPrice  # è¿”å›æ€»ä»·æ ¼

    # path = r"P:\MyNutstore\StarryNight299792458\è£…å¤‡\ä¸ªäººè®¡ç®—æœº\0_æ•´æœº\é…ç½®å•\4_Intel Core i7-12700K\Intel Core i7-12700K(å¸‚åœºä»·).md"
    path = getCMDargv()  # è·å–.mdæ–‡ä»¶è·¯å¾„
    totalPrice = diyPC(path)  # è°ƒç”¨diyPCå‡½æ•°è§£æ.mdæ–‡ä»¶å¹¶è®¡ç®—æ€»ä»·æ ¼
    pyperclip.copy(totalPrice)  # å°†æ€»ä»·æ ¼å¤åˆ¶åˆ°å‰ªè´´æ¿
    print(totalPrice)  # æ‰“å°æ€»ä»·æ ¼


# priceCalculate()


def generate_markdown_link(text, anchorTextPattern=""):
    """Markdownè¶…é“¾æ¥ç”Ÿæˆ(è‡ªåŠ¨æ ¹æ®urlç”ŸæˆMarkdownæ ¼å¼çš„è¶…é“¾æ¥,åŒ…å«æ–‡å­—å’Œé“¾æ¥)
    :param text: è¾“å…¥çš„æ–‡æœ¬,ä¸€å®šè¦åŒ…å«url
    :return : Markdownæ ¼å¼çš„è¶…é“¾æ¥
    """
    # import re  # å¯¼å…¥æ­£åˆ™è¡¨è¾¾å¼åº“
    # import requests  # å¯¼å…¥HTTPè¯·æ±‚åº“
    # from bs4 import BeautifulSoup  # å¯¼å…¥BeautifulSoup4åº“
    # text = '''PassMark Software - PC Benchmark and Test Software
    # https://www.passmark.com/'''
    # text = 'https://www.passmark.com/'
    text = re.sub(r"\r", "", text)  # windowsçš„æ¢è¡Œç¬¦æ˜¯\r\n ï¼Œæå‰åˆ é™¤\r
    resultMatch = re.search(
        r"(https?)://[\w\-]+(\.[\w\-]+)+(/[\w\- ./?%&=]*)?", text
    )  # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æœç´¢æ–‡æœ¬ä¸­çš„URL
    anchorText = [t for t in text.split("\n") if t]  # æŠŠæ–‡æœ¬æŒ‰è¡Œåˆ†å‰²ï¼Œå¹¶ä¸”å»é™¤ç©ºå­—ç¬¦ä¸²
    try:  # å°è¯•æå–URLå’Œé“¾æ¥æ–‡æœ¬
        url = resultMatch.group()  # æå–URL
        if len(anchorText) > 1:  # åˆ¤æ–­é“¾æ¥æ–‡æœ¬æ˜¯å¦ä¸ºå¤šè¡Œ
            anchorText = anchorText[0]  # å–ç¬¬ä¸€è¡Œä½œä¸ºé“¾æ¥æ–‡æœ¬
        else:
            headers = {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.55 Safari/537.36"
                # å®šä¹‰è¯·æ±‚å¤´éƒ¨
            }
            response = requests.get(url, headers=headers)  # å‘å‡ºHTTPè¯·æ±‚
            soup = BeautifulSoup(response.text, "lxml")  # è§£æè¯·æ±‚çš„HTMLå†…å®¹
            anchorText = soup.head.title.text  # æå–æ ‡é¢˜ä½œä¸ºé“¾æ¥æ–‡æœ¬

        if anchorTextPattern:
            # åŒ¹é…æŒ‡å®šæ–‡æœ¬
            anchorTextList = re.findall(anchorTextPattern, anchorText)
            if len(anchorTextList) > 1:
                # print(anchorTextList)
                # ['ã€å¾®æ˜ŸMAG B550M MORTAR MAX WIFI', 'ã€è¡Œæƒ… æŠ¥ä»· ä»·æ ¼ è¯„æµ‹']
                # ç¬¬ä¸€ä¸ªåŒ¹é…åˆ°çš„æ–‡æœ¬ä½œä¸ºæ ‡é¢˜
                anchorText = anchorTextList[0][1:]

        markdownHref = f"[{anchorText}]({url} )"  # ç”ŸæˆMarkdownæ ¼å¼è¶…é“¾æ¥
    except AttributeError as err:
        markdownHref = ""
        print("æ²¡æœ‰æ‰¾åˆ°URL")  # æ‰“å°æç¤ºä¿¡æ¯
    return markdownHref  # è¿”å›Markdownæ ¼å¼è¶…é“¾æ¥
    """
    re: æ­£åˆ™è¡¨è¾¾å¼
    requests: å‘å‡ºHTTPè¯·æ±‚
    bs4:BeautifulSoup4 ç½‘é¡µè§£æ
    """


# text = 'https://www.passmark.com/'
# print(generate_markdown_link(text))


def weatherForecast(
    cityName="å—å±±",
    apikey="e1f15c34780348edba48ae1c24dbda46",
    daysAfterToday=1,
):
    """
    import requests
    import json
    import time
    GeoAPIè¯·æ±‚URL:
    https://geoapi.qweather.com/v2/city/lookup?location=å—å±±&adm=æ·±åœ³&key=è¿™é‡Œæ›¿æ¢æˆä½ çš„key
    å…è´¹è®¢é˜…è¯·æ±‚URL:
    https://devapi.qweather.com/v7/weather/7d?location=101280604&key=è¿™é‡Œæ›¿æ¢æˆä½ çš„key
    ç©ºæ°”è´¨é‡æ¯æ—¥é¢„æŠ¥:
    https://devapi.qweather.com/v7/air/5d?location=101280604&key=è¿™é‡Œæ›¿æ¢æˆä½ çš„key
    """

    def city_lookup(cityName, apikey, administrator=""):
        """
        requests.get(url, params=params)æ˜¯ä½¿ç”¨HTTP GETæ–¹æ³•å‘æŒ‡å®šçš„urlå‘é€è¯·æ±‚ï¼Œ
        å¹¶å°†å‚æ•°paramsä½œä¸ºæŸ¥è¯¢å­—ç¬¦ä¸²é™„åŠ åˆ°urlä¸­ã€‚è¿™æ˜¯ä¸€ç§å¸¸è§çš„å‘APIå‘é€è¯·æ±‚çš„æ–¹æ³•ã€‚
        ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨&è¿æ¥å‚æ•°ï¼Œä½†æ˜¯è¿™æ ·çš„è¯ï¼Œä½ éœ€è¦è‡ªå·±æ„å»ºæŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œè¿™å¯èƒ½ä¼šæ¯”è¾ƒéº»çƒ¦ã€‚
        ä½¿ç”¨paramså‚æ•°å¯ä»¥è®©requestsåº“è‡ªåŠ¨æ„å»ºæŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œè¿™æ ·æ›´åŠ æ–¹ä¾¿ã€‚

        response.json()å’Œjson.loads(response.text)çš„æ•ˆæœæ˜¯ä¸€æ ·çš„ï¼Œ
        éƒ½æ˜¯å°†APIè¿”å›çš„jsonæ ¼å¼çš„å­—ç¬¦ä¸²è½¬æ¢ä¸ºpythonå¯¹è±¡ã€‚
        response.json()æ˜¯requestsåº“æä¾›çš„ä¸€ä¸ªæ–¹æ³•ï¼Œå¯ä»¥ç›´æ¥å°†APIè¿”å›çš„jsonæ ¼å¼çš„å­—ç¬¦ä¸²
        è½¬æ¢ä¸ºpythonå¯¹è±¡ã€‚
        json.loads(response.text)æ˜¯ä½¿ç”¨pythonå†…ç½®çš„jsonåº“å°†jsonæ ¼å¼çš„å­—ç¬¦ä¸²è½¬æ¢ä¸ºpythonå¯¹è±¡ã€‚
        """
        url = "https://geoapi.qweather.com/v2/city/lookup"
        parameters = {
            "location": cityName,
            "adm": administrator,
            "key": apikey,
        }
        header = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.55 Safari/537.36"
        }
        response = requests.get(url, headers=header, params=parameters)
        #     print(response.text)
        data = json.loads(
            response.text
        )  # json.loads() å°†jsonæ ¼å¼çš„å­—ç¬¦ä¸²è½¬æ¢ä¸ºpythonå¯¹è±¡

        cityID = data["location"][0]["id"]  #  "id":"101280604",
        cityAdm = data["location"][0]["adm2"]  #  "adm2":"æ·±åœ³",
        # è¯¥åœ°åŒºçš„å¤©æ°”é¢„æŠ¥ç½‘é¡µé“¾æ¥ï¼Œä¾¿äºåµŒå…¥ä½ çš„ç½‘ç«™æˆ–åº”ç”¨
        # https://www.qweather.com/weather/nanshan-101280604.html
        fxLink = data["location"][0]["fxLink"]
        return cityID, cityAdm, fxLink

    def getQweatherTextEmoji(iconCodeDay):
        # træ˜¯HTMLè¡¨æ ¼ä¸­çš„è¡Œï¼Œå…¨ç§°æ˜¯Table Row
        # tdæ˜¯HTMLè¡¨æ ¼ä¸­çš„å•å…ƒæ ¼ï¼Œå…¨ç§°æ˜¯Table Data
        # thæ˜¯HTMLè¡¨æ ¼ä¸­çš„è¡¨å¤´å•å…ƒæ ¼ï¼Œå…¨ç§°æ˜¯Table Header
        """
        å½“æˆ‘ä»¬ä½¿ç”¨Pythonè®¿é—®ç½‘é¡µæ—¶ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨requestsåº“æ¥å‘é€HTTPè¯·æ±‚å¹¶è·å–å“åº”ã€‚
        åœ¨è¿™ä¸ªä»£ç ä¸­ï¼Œurlæ˜¯æˆ‘ä»¬è¦è®¿é—®çš„ç½‘å€ã€‚
        æˆ‘ä»¬ä½¿ç”¨requests.get(url)æ–¹æ³•æ¥å‘é€GETè¯·æ±‚å¹¶è·å–å“åº”ã€‚
        å“åº”æ˜¯ä¸€ä¸ªåŒ…å«æœåŠ¡å™¨å“åº”çš„å¯¹è±¡ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨response.contentå±æ€§æ¥è·å–å“åº”çš„å†…å®¹ã€‚
        å“åº”çš„å†…å®¹é€šå¸¸æ˜¯HTMLæˆ–JSONæ ¼å¼çš„æ•°æ®ã€‚åœ¨è¿™ä¸ªä»£ç ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨BeautifulSoupåº“æ¥è§£æHTMLå¹¶è·å–æ•°æ®ã€‚
        soupæ˜¯ä¸€ä¸ªåŒ…å«HTMLæ–‡æ¡£çš„å¯¹è±¡ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨soup.find_allæ–¹æ³•æ¥æŸ¥æ‰¾HTMLå…ƒç´ ã€‚
        """
        import requests
        from bs4 import BeautifulSoup

        url = "https://dev.qweather.com/docs/resource/icons/"
        response = requests.get(url)
        soup = BeautifulSoup(response.content, "html.parser")

        # <tr>
        # <th>å›¾æ ‡ä»£ç </th>  tds[0]
        # <th>å¤©æ°”</th>      tds[1]
        # <th>ç™½å¤©</th>      tds[2]
        # <th>å¤œæ™š</th>      tds[3]
        # </tr>
        # print(soup.find_all('tr')[1:-9])
        textEmoji = {}
        emoji = {
            "100": "â˜€ï¸",
            "101": "ğŸŒ¥ï¸",
            "102": "ğŸŒ¤ï¸",
            "103": "ğŸŒ¤ï¸â˜€ï¸",
            "104": "â˜ï¸",
            "300": "ğŸŒ¦ï¸",
            "301": "ğŸŒ§ï¸",
            "302": "â›ˆï¸",
            "303": "ğŸŒ©ï¸",
            "304": "ğŸŒ©ï¸â„ï¸",
            "305": "ğŸŒ§ï¸",
            "306": "ğŸŒ§ï¸",
            "307": "ğŸŒ§ï¸â˜”",
            "308": "ğŸŒ§ï¸â˜”",
            "309": "ğŸŒ§ï¸",
            "310": "ğŸŒ§ï¸â˜”",
            "311": "ğŸŒ§ï¸â˜”",
            "312": "ğŸŒ§ï¸â˜”",
            "313": "ğŸŒ§ï¸â„ï¸",
            "314": "ğŸŒ§ï¸",
            "315": "ğŸŒ§ï¸",
            "316": "ğŸŒ§ï¸â˜”",
            "317": "ğŸŒ§ï¸â˜”",
            "318": "ğŸŒ§ï¸â˜”",
            "399": "ğŸŒ§ï¸",
            "400": "â„ï¸",
            "401": "â„ï¸",
            "402": "â„ï¸",
            "403": "â„ï¸",
            "404": "ğŸŒ¨ï¸",
            "405": "ğŸŒ¨ï¸ğŸŒ§ï¸",
            "406": "ğŸŒ¨ï¸ğŸŒ§ï¸",
            "407": "â„ï¸",
            "408": "â„ï¸",
            "409": "â„ï¸",
            "410": "â„ï¸",
            "499": "â„ï¸",
            "500": "ğŸŒ«ï¸",
            "501": "ğŸŒ«ï¸",
            "502": "ğŸŒ«ï¸",
            "503": "ğŸŒªï¸",
            "504": "ğŸŒªï¸",
            "507": "ğŸŒªï¸",
            "508": "ğŸŒªï¸",
            "509": "ğŸŒ«ï¸",
            "510": "ğŸŒ«ï¸",
            "511": "ğŸŒ«ï¸",
            "512": "ğŸŒ«ï¸",
            "513": "ğŸŒ«ï¸",
            "514": "ğŸŒ«ï¸",
            "515": "ğŸŒ«ï¸",
            "900": "ğŸ”¥",
            "901": "â„ï¸",
            "999": "â“",
        }
        for item in soup.find_all("tr")[1:-9]:
            tds = item.find_all("td")
            if tds[2].text == "âœ…":  # åªéœ€è¦ç™½å¤©çš„å›¾æ ‡
                #         print(tds)
                textEmoji[tds[0].text] = (
                    tds[1].text,
                    emoji.get(tds[0].text, "defaultâ“"),
                )
        #     print(len(emoji),len(textEmoji))
        return textEmoji.get(iconCodeDay, "defaultâ“")

    def airDailyForecast(cityID, apikey, daysAfterToday):
        if daysAfterToday > 4:  # ç©ºæ°”è´¨é‡é¢„æŠ¥5å¤©(0,1,2,3,4) ï¼Œå¤©æ°”é¢„æŠ¥7å¤©
            daysAfterToday = 4
        url = "https://devapi.qweather.com/v7/air/5d"
        parameters = {
            "location": cityID,
            "key": apikey,
        }
        response = requests.get(url, params=parameters)
        #     print(response.text)
        data = json.loads(
            response.text
        )  # json.loads() å°†jsonæ ¼å¼çš„å­—ç¬¦ä¸²è½¬æ¢ä¸ºpythonå¯¹è±¡
        weather = data["daily"][daysAfterToday]  # dict
        airQuality = weather["aqi"]  # "aqi": "46",
        airQualityLevel = weather["category"]  # "category": "ä¼˜",

        return airQuality, airQualityLevel

    def weather_now(cityID, apikey, fxLink):
        import requests
        from bs4 import BeautifulSoup

        url = fxLink
        response = requests.get(url)
        soup = BeautifulSoup(response.content, "html.parser")
        current_abstract = soup.find_all("div", class_="current-abstract")[
            0
        ].text.strip()
        return current_abstract

    if daysAfterToday > 6:  # å¤©æ°”é¢„æŠ¥7å¤©(0,1,2,3,4,5,6)
        daysAfterToday = 6
    # æ·±åœ³å—å±±æ˜å¤©
    cityID, cityAdm, fxLink = city_lookup(cityName, apikey)
    # å½“å‰å®æ—¶å¤©æ°”æ‘˜è¦
    current_abstract = weather_now(cityID, apikey, fxLink)
    # å¤©æ°”é¢„æŠ¥ï¼šå¤šäº‘
    url = "https://devapi.qweather.com/v7/weather/7d"
    params = {
        "key": apikey,
        "location": cityID,
    }
    response = requests.get(url, params=params)
    data = json.loads(response.text)
    #     print(response.text)

    weather = data["daily"][daysAfterToday]  # dict
    weatherTextEmoji = getQweatherTextEmoji(weather["iconDay"])  # "iconDay": "100",

    # daily.fxDate é¢„æŠ¥æ—¥æœŸ
    daysAfterTodayList = [
        "ä»Šå¤©",
        "æ˜å¤©",
        "åå¤©",
    ]
    if daysAfterToday < 3:
        fxDate = daysAfterTodayList[daysAfterToday]
    else:
        fxDate = str(weather["fxDate"][-2:]) + "æ—¥"

    weatherForecast_textDay = weather["textDay"]
    tempMax = weather["tempMax"]
    tempMin = weather["tempMin"]
    windDir = weather["windDirDay"]
    windScale = weather["windScaleDay"]

    # ç©ºæ°”è´¨é‡æ¯æ—¥é¢„æŠ¥
    airQuality, airQualityLevel = airDailyForecast(cityID, apikey, daysAfterToday)
    # å¤©æ°”æ–‡å­—åŠ å›¾æ ‡weatherTextEmoji,('å¤§é›¨', 'ğŸŒ§ï¸â˜”')
    # æ·±åœ³å—å±±æ˜å¤©å¤šäº‘ï¼Œé˜´æ™´ä¹‹é—´ï¼Œè°¨é˜²ç´«å¤–çº¿ä¾µæ‰°ï¼Œ20â„ƒåˆ°26â„ƒï¼Œä¸œå—é£3çº§ï¼Œç©ºæ°”è´¨é‡46ä¼˜
    formatString = f"{cityAdm}{cityName}{fxDate}{weatherTextEmoji[0]}ï¼Œ{tempMax}â„ƒåˆ°{tempMin}â„ƒï¼Œ{windDir}{windScale}çº§ï¼Œç©ºæ°”è´¨é‡{airQuality}{airQualityLevel}"
    return weatherTextEmoji[1], formatString, current_abstract


def searchPoetry(cacheTime_Minutes=0.1):
    """
    æœç´¢å¤è¯—æ–‡ç½‘ç»œèµ„æ–™å¹¶è§£æå†…å®¹
    print: æ‰“å°
    requests: ç½‘ç»œè¯·æ±‚åº“
    bs4:BeautifulSoup4 è§£æåº“
    os: æ“ä½œç³»ç»Ÿæ¥å£
    re: æ­£åˆ™è¡¨è¾¾å¼åº“
    json: ç”¨äºpythonå¯¹è±¡ä¸JSONæ ¼å¼æ•°æ®çš„è½¬æ¢
    time:æ—¶é—´å¤„ç†åº“
    """
    # import requests
    # from bs4 import BeautifulSoup
    # import os
    # import re
    # import json
    # import time

    # å‘é€ç½‘ç»œè¯·æ±‚æœç´¢å¤è¯—æ–‡

    # ä¼ªè£…è¯·æ±‚å¤´
    headers = {
        "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36",
    }
    url = "https://so.gushiwen.cn/shiwenv_630c04c81858.aspx"
    # æœç´¢å…³é”®è¯
    keyword = "è§‚æ²§æµ·"
    # è¾“å…¥æœç´¢å…³é”®è¯
    keyword = input("è¯·è¾“å…¥æœç´¢å…³é”®è¯:")

    # æ‹¼æ¥æœç´¢URL
    searchURL = (
        f"https://so.gushiwen.cn/search.aspx?value={keyword}&valuej={keyword[0]}"
    )
    # æå–URLå‰ç¼€
    prefixURL = re.match(r"(http|https)://+[a-zA-Z\.\-0-9]+", url)[0]

    # ç¼“å­˜å‡½æ•°
    def cacheFile(url, filename, cacheTime_Minutes):
        # å¦‚æœå­˜åœ¨ç¼“å­˜æ–‡ä»¶åˆ™åˆ¤æ–­æ˜¯å¦æ›´æ–°,å¦åˆ™ç›´æ¥å†™å…¥æ–°å†…å®¹
        if os.path.isfile(filename):  # å¦‚æœ å­˜åœ¨ç¼“å­˜æ–‡ä»¶
            nowTime = time.time()
            modifyTime = os.path.getmtime(filename)
            if nowTime - modifyTime > int(cacheTime_Minutes * 60):
                # å¦‚æœç¼“å­˜æ–‡ä»¶è¶…è¿‡1åˆ†é’Ÿæœªæ›´æ–°ï¼Œåˆ™å¼ºåˆ¶æ›´æ–°
                # print(time.localtime(modifyTime))
                with open(filename, "wb") as f:
                    response = requests.get(url, headers=headers)
                    #             print(f"{type(response.text)=}")  # <class 'str'>
                    #             print(f"{type(response.content)=}") # <class 'bytes'>
                    f.write(response.content)
            else:  # ç¼“å­˜æ–‡ä»¶æ›´æ–°äº3åˆ†é’Ÿä¹‹å†…ï¼›åˆ™è¯»å–
                with open(filename, "rb") as f:
                    response = f.read()
        else:  # å¦åˆ™ä¸å­˜åœ¨ç¼“å­˜æ–‡ä»¶ï¼Œåˆ™å†™å…¥
            with open(filename, "wb") as f:
                response = requests.get(url, headers=headers)
                #             print(f"{type(response.text)=}")  # <class 'str'>
                #             print(f"{type(response.content)=}") # <class 'bytes'>
                f.write(response.content)
        if isinstance(response, bytes):
            responseContent = response.decode("utf-8")
            htmldata = BeautifulSoup(responseContent, "lxml")
        else:
            htmldata = BeautifulSoup(response.text, "lxml")
        return htmldata

    # åˆ›å»ºç¼“å­˜æ–‡ä»¶å¤¹poetryCache
    # print(os.getcwd())
    os.makedirs(os.path.join(os.getcwd(), "poetryCache"), exist_ok=True)
    # è°ƒç”¨ç¼“å­˜æ–‡ä»¶å‡½æ•°
    htmldata = cacheFile(
        searchURL,
        os.path.join(
            os.getcwd(),
            "poetryCache",
            "gushiwen.html",
        ),
        cacheTime_Minutes=cacheTime_Minutes,
    )
    #     print(htmldata)
    # print(help(htmldata.find))
    # print(help(htmldata.findAll))
    # print(len(htmldata.findAll('div', class_="cont")))
    poetryDict = dict()

    # å­˜å‚¨æœç´¢å…³é”®è¯
    poetryDict["keyword"] = keyword

    # å­˜å‚¨æœç´¢ç»“æœ
    poetryDict["poems"] = []

    # éå†æ¯é¦–è¯—è¯ä¿¡æ¯
    for poetry in htmldata.findAll("div", class_="cont"):
        poetryMultiTag_p = poetry.findAll("p")
        length = len(poetryMultiTag_p)
        #         print('-------')
        #         print(f"{length=}")
        if poetryMultiTag_p and length > 1:

            #             print(f"{poetryMultiTag_p=}")

            verseDict = dict()
            # æå–è¯—è¯æ ‡é¢˜
            verseDict["poetryTitle"] = "".join(poetryMultiTag_p[0].text.split())
            # æå–è¯—è¯è¯¦æƒ…é¡µé“¾æ¥
            verseDict["poetry_href"] = prefixURL + poetryMultiTag_p[0].a.get("href")
            NDIndex = 1
            for i in range(length):
                #                 print(f"poetryMultiTag_p[{i}]:{poetryMultiTag_p[i]}")
                ahrefValue = poetryMultiTag_p[i].a.get("href")
                #                 print(f"-----{ahrefValue=}-----")
                if isinstance(ahrefValue, list):
                    ahrefValue = ahrefValue[0]
                #                 -----None-----
                #                 -----['source']-----
                if ahrefValue and re.search(r"authorv", ahrefValue):
                    NDIndex = i
                    break
            # æå–åå­—å’Œæœä»£
            NameAndDynasty = poetryMultiTag_p[NDIndex].text.split()[0]
            #             print(NameAndDynasty)
            pattern = r"(\[|ã€”).*(ã€•|\])"
            reResult = re.search(pattern, NameAndDynasty)
            # æå–è¯—äººå§“å
            verseDict["authorName"] = reResult.group()
            # æå–è¯—äººæœä»£
            verseDict["authorDynasty"] = NameAndDynasty[: reResult.start()]

            # å­˜å‚¨æ¯é¦–è¯—è¯ä¿¡æ¯
            poetryDict["poems"].append(verseDict)
    # pythonå¯¹è±¡è½¬æ¢ä¸ºjsonæ–‡æœ¬
    JavaScriptObjectNotation = json.dumps(poetryDict)
    # print(JavaScriptObjectNotation)
    # print(poetryDict)

    # éå†è¾“å‡ºæœç´¢ç»“æœ
    poems = poetryDict["poems"]
    for i in range(len(poems)):
        print(
            f"""{i+1}. {poems[i]['poetryTitle']} --- {poems[i]['authorName']+poems[i]['authorDynasty']}"""
        )

    try:
        select = int(input("è¯·è¾“å…¥ç¼–å·é€‰æ‹©è¯—è¯:"))
    except ValueError:
        select = 1
    select -= 1

    # æå–é€‰æ‹©çš„è¯—è¯è¯¦æƒ…é¡µé“¾æ¥
    url = poems[select]["poetry_href"]

    # è§£æè¯¦æƒ…é¡µå†…å®¹
    htmldata = cacheFile(
        url,
        os.path.join(
            os.getcwd(), "poetryCache", f"{poems[select]['poetryTitle']}.html"
        ),
        cacheTime_Minutes=cacheTime_Minutes,
    )

    # å­˜å‚¨è¯¦æƒ…é¡µå†…å®¹
    verseContentDict = dict()

    # æå–æ ‡é¢˜
    verseContentDict["Title"] = poems[select]["poetryTitle"]

    # æå–è¯—äººå§“å
    verseContentDict["Author"] = poems[select]["authorName"]

    # æå–è¯—äººæœä»£
    verseContentDict["Dynasty"] = poems[select]["authorDynasty"]
    verseNumber = re.findall(r"(?<=_)[^\.]+", poems[select]["poetry_href"])[0]
    verseId = f"""contson{verseNumber}"""
    try:
        # æå–è¯—è¯å†…å®¹
        verseContent = str((htmldata.findAll("div", class_="contson", id=verseId)[0]).p)
        if verseContent:
            raise IndexError

        # å»é™¤<p> </p>
        verseContent = re.sub(r"(\<p\>|\<\/p\>)", "", verseContent)

    # å¼‚å¸¸å¤„ç†
    except IndexError:
        verseContent = str((htmldata.findAll("div", class_="contson", id=verseId)[0]))
        verseContent = re.sub(
            r"(\<div.*\>|\<\/div\>)", "", verseContent
        )  # å»é™¤<div> </div>

    verseContent = re.sub(r"\s", "", verseContent)  # å»é™¤ç©ºç™½å­—ç¬¦
    verseContent = re.sub(r"(\<p\>)", "", verseContent)  # å»é™¤<p>
    verseContent = re.sub(r"\<br\/\>", "\n", verseContent)  # æ›¿æ¢<br/>ä¸ºæ¢è¡Œ
    verseContent = re.sub(r"\<\/p\>", "\n", verseContent)  # æ›¿æ¢</p>ä¸ºæ¢è¡Œ

    # å­˜å‚¨è¯—è¯å†…å®¹
    verseContentDict["Content"] = verseContent
    return "\n" + "\n".join(verseContentDict.values())


def todayPoetry():
    """
    ä»Šæ—¥è¯—è¯
    """
    # import json, requests
    # from bs4 import BeautifulSoup

    url = "https://www.1saying.com/"
    header = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.55 Safari/537.36"
    }
    response = requests.get(url, headers=header)
    soup = BeautifulSoup(response.content, "html.parser")
    today_poetry_container = soup.find_all("div", class_="today-poetry-container")
    # print(today_poetry_container,type(today_poetry_container))
    today_poetry = str(today_poetry_container[0].text)
    return "\n".join(today_poetry.split())


# ğŸ“œ å®šä¹‰æ¨¡å—çš„å…¬å¼€æ¥å£
__all__ = [
    # ğŸ” éå†å…¨å±€å˜é‡å­—å…¸
    name
    for name, obj in globals().items()
    # âœ… åªé€‰æ‹©ä¸ä»¥ä¸‹åˆ’çº¿å¼€å¤´çš„å˜é‡
    if not name.startswith("_")
    # âœ… æ’é™¤ç±»å‹ä¸ºæ¨¡å—çš„å¯¹è±¡
    and not isinstance(obj, types.ModuleType)
    # âœ… æ’é™¤åœ¨é›†åˆé‡Œçš„å˜é‡
    and name not in {"wantobjects", "types"}
]


if __name__ == "__main__":
    print(todayPoetry())
    print(searchPoetry(cacheTime_Minutes=60))
    for i in range(3):
        print(
            weatherForecast(
                cityName="å—å±±",
                apikey="e1f15c34780348edba48ae1c24dbda46",
                daysAfterToday=i,
            )
        )
        time.sleep(1)
