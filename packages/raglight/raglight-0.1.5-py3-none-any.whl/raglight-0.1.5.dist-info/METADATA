Metadata-Version: 2.3
Name: raglight
Version: 0.1.5
Summary: RAGLight is a lightweight and modular Python library for implementing Retrieval-Augmented Generation (RAG). It enhances the capabilities of Large Language Models (LLMs) by combining document retrieval with natural language inference.
License: Designed for simplicity and flexibility, RAGLight provides modular components to easily integrate various LLMs, embeddings, and vector stores, making it an ideal tool for building context-aware AI solutions. âœ¨MIT
Author: Bessouat40
Author-email: roman.bessouat@orange.fr
Requires-Python: >=3.11,<4.0
Classifier: License :: Other/Proprietary License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Requires-Dist: langchain_chroma (==0.1.4)
Requires-Dist: langchain_community (==0.3.11)
Requires-Dist: langchain_core (==0.3.24)
Requires-Dist: langchain_huggingface (==0.1.2)
Requires-Dist: langchain_ollama (==0.2.1)
Requires-Dist: langchain_text_splitters (==0.3.2)
Requires-Dist: langgraph (==0.2.59)
Requires-Dist: ollama (==0.4.0)
Requires-Dist: typing (==3.7.4.3)
Requires-Dist: typing_extensions (==4.12.2)
Description-Content-Type: text/markdown

# RAGLight

[![PyPI version](https://badge.fury.io/py/raglight.svg)](https://badge.fury.io/py/raglight)
![License](https://img.shields.io/github/license/Bessouat40/RAGLight)
![PyPI Downloads](https://img.shields.io/pypi/dm/raglight)

**RAGLight** is a lightweight and modular Python library for implementing **Retrieval-Augmented Generation (RAG)**. It enhances the capabilities of Large Language Models (LLMs) by combining document retrieval with natural language inference.

Designed for simplicity and flexibility, RAGLight provides modular components to easily integrate various LLMs, embeddings, and vector stores, making it an ideal tool for building context-aware AI solutions. âœ¨

---

## Features ðŸ”¥

- ðŸŒ **Embeddings Model Integration**: Plug in your preferred embedding models (e.g., HuggingFace `all-MiniLM-L6-v2`) for compact and efficient vector embeddings.
- ðŸ§™ðŸ½ **LLM Agnostic**: Seamlessly integrates with different LLMs, such as `llama3` or custom providers, for natural language inference.
- âš–ï¸ **RAG Pipeline**: Combines document retrieval and language generation in a unified workflow.
- ðŸ–‹ï¸ **Flexible Document Support**: Ingest and index various document types (e.g., PDF, TXT, DOCX).
- ðŸ› ï¸ **Extensible Architecture**: Easily swap vector stores, embedding models, or LLMs to suit your needs.

---

## Import library ðŸ› ï¸

If you want to install library, use :

```bash
pip install raglight
```

## Installation without import ðŸ› ï¸

- Install RAGLight directly from PyPI:

```bash
poetry install
```

Then you can run your code :

```bash
poetry run python my_code.py
```

- If you want to use `pip install` :

```bash
pip install -r requirements.txt
```

---

## Quick Start ðŸš€

### **1. Configure Your Pipeline**

Set up the components of your RAG pipeline:

```python
from raglight.rag.builder import Builder
from raglight.config.settings import Settings

rag = Builder() \
    .with_embeddings(Settings.HUGGINGFACE, model_name=model_embeddings) \
    .with_vector_store(Settings.CHROMA, persist_directory=persist_directory, collection_name=collection_name) \
    .with_llm(Settings.OLLAMA, model_name=model_name, system_prompt_file=system_prompt_directory) \
    .build_rag()
```

### **2. Ingest Documents**

Use the pipeline to ingest documents into the vector store:

```python
rag.vector_store.ingest(file_extension='**/*.pdf', data_path='./data')
```

### **3. Query the Pipeline**

Retrieve and generate answers using the RAG pipeline:

```python
response = rag.question_graph("How can I optimize my marathon training?")
print(response)
```

---

## Advanced Configuration âš™ï¸

### Environment Variables

Configure the pipeline with environment variables for better modularity:

```bash
export PERSIST_DIRECTORY=./vectorstore
export MODEL_EMBEDDINGS=all-MiniLM-L6-v2
export MODEL_NAME=llama3
```

You can also define these in a `.env` file:

```bash
PERSIST_DIRECTORY=./vectorstore
MODEL_EMBEDDINGS=all-MiniLM-L6-v2
MODEL_NAME=llama3
```

---

## TODO

- [ ] **Feature**: Add the possibility to use custom pipelines while ingesting data into the Vector Store.
- [ ] **Feature**: Add support for new Vector Stores (e.g., FAISS, Weaviate, Milvus).
- [ ] **Feature**: Integrate new LLM providers (e.g., VLLM, HuggingFace, GPT-Neo).

---

ðŸš€ **Get started with RAGLight today and build smarter, context-aware AI solutions!**

