import abc
import threading
from _typeshed import Incomplete
from abc import ABC, abstractmethod
from tlc.core.objects.tables.system_tables.indexing import _BlacklistExceptionHandler, _ScanUrl, _UrlIndex
from tlc.core.tlc_core_threadpool import submit_future as submit_future
from tlc.core.url import Url as Url
from typing import Iterator

logger: Incomplete

class _UrlIndexingWorker(ABC, metaclass=abc.ABCMeta):
    def __init__(self, data_event: threading.Event, blacklist_config: list[_BlacklistExceptionHandler] | None, tag: str = '', stop_event: threading.Event | None = None, extensions: list[str] | None = None) -> None: ...
    @property
    def data_event(self) -> threading.Event: ...
    def touch(self) -> None:
        """Update the timestamp of the last activity to prevent the indexer from going idle."""
    @abstractmethod
    def set_index(self, new_index: dict[Url, _UrlIndex]) -> None:
        """Set the new index and notify any waiting threads.

        This method will also reset the idle timeout."""
    @abstractmethod
    def get_index(self) -> dict[Url, _UrlIndex]:
        """Retrieve the last index generated by the worker."""
    @abstractmethod
    def request_complete_reindex(self) -> None: ...
    def start(self) -> None: ...
    def stop(self) -> None: ...
    @property
    @abstractmethod
    def is_running(self) -> bool: ...
    @abstractmethod
    def wait_for_complete_reindex(self, timeout: float | None = None) -> bool:
        """Wait until an indexing cycle has completed.

        :return: True if the cycle completed within the timeout, False otherwise.
        """
    def join(self, timeout: float | None = None) -> None: ...
    @abstractmethod
    def add_scan_url(self, url_config: _ScanUrl) -> None:
        """Adds a new URL to the list of URLs to be scanned by the indexer.

        The new URL is guaranteed to be scanned in the next cycle.
        The new URL is not visible in the get_scan_urls() method until the next cycle.
        """
    @abstractmethod
    def remove_scan_url(self, url_config: _ScanUrl) -> None:
        """Removes a URL from the list of URLs to be scanned by the indexer.

        The new URL is not guaranteed to be removed until the next cycle.
        """
    @abstractmethod
    def get_scan_urls(self) -> list[_ScanUrl]: ...

class _CrawlUrlIndexer(threading.Thread, _UrlIndexingWorker):
    """An indexer that crawls a directory and indexes the files in it.

    This is an indexer that repeatedly performs a scan of its directories and indexes the files in it. It runs in
    a separate thread to allow for asynchronous scanning, but has an idle overhead. The indexer can be stopped and
    started again.

    The actual scanning of the directory is done by ScanIterators that wrap around URL-adapters.
    """
    def __init__(self, interval: float, data_event: threading.Event, blacklist_config: list[_BlacklistExceptionHandler], tag: str = '', stop_event: threading.Event | None = None, extensions: list[str] | None = None) -> None: ...
    def add_scan_url(self, url_config: _ScanUrl) -> None:
        """Add a scan URL to this crawl indexer

        The indexer will determine the best scan iterator for the given URL and if this type is not already running it
        will create a new one. If the indexer is running, the URL will be added to the pending list and processed in the
        next cycle.


        :param url_config: The URL configuration to add
        """
    def remove_scan_url(self, url_config: _ScanUrl) -> None: ...
    def handle_pending_scan_urls(self) -> None: ...
    def get_scan_urls(self) -> list[_ScanUrl]: ...
    def run(self) -> None:
        """Method representing the thread's activity.

        Do not call this method directly. Use the start() method instead, which will in turn call this method.
        """
    def handle_scan(self, new_scan: Iterator[_UrlIndex]) -> dict[Url, _UrlIndex] | None:
        """Checks a newly scanned index for changes and returns an optionally updated index"""
    def request_complete_reindex(self) -> None: ...
    def wait_for_complete_reindex(self, timeout: float | None = None) -> bool: ...
    def stop(self) -> None:
        """Method to signal the thread to stop its activity.

        This doesn't terminate the thread immediately, but flags it to exit when it finishes its current iteration.
        """
    def join(self, timeout: float | None = None) -> None:
        """Wait for the thread to join.

        This method will block until the thread has finished its current iteration and is ready to join.
        """
    @property
    def is_running(self) -> bool: ...
    def get_index(self) -> dict[Url, _UrlIndex]: ...
    def set_index(self, new_index: dict[Url, _UrlIndex]) -> None:
        """Set the new index and notify any waiting threads.

        The new data may have removed entries or even be empty.
        """

class _StaticUrlIndexer(_UrlIndexingWorker):
    """An indexer that scans URLs that are considered static."""
    def __init__(self, data_event: threading.Event, blacklist_config: list[_BlacklistExceptionHandler], tag: str = '', stop_event: threading.Event | None = None, extensions: list[str] | None = None) -> None: ...
    def get_index(self) -> dict[Url, _UrlIndex]: ...
    def add_scan_url(self, url_config: _ScanUrl) -> None: ...
    def remove_scan_url(self, url_config: _ScanUrl) -> None: ...
    @property
    def is_running(self) -> bool: ...
    def stop(self) -> None: ...
    def start(self) -> None: ...
    def request_complete_reindex(self) -> None: ...
    def wait_for_complete_reindex(self, timeout: float | None = None) -> bool: ...
    def get_scan_urls(self) -> list[_ScanUrl]: ...
    def set_index(self, new_index: dict[Url, _UrlIndex]) -> None: ...
