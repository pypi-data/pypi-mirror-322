{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional, Iterator\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import whisper\n",
    "import webdataset as wds\n",
    "from torch.utils.data import IterableDataset\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "\n",
    "def load_whisper_dataset(\n",
    "    dataset_dir: str,\n",
    "    txt_label: str = \"transcription\",\n",
    "    model: str = \"medium\",\n",
    "    language: str = \"vi\",\n",
    "    weight: float = 1,\n",
    "    validation: bool = False,\n",
    "    num_samples: Optional[int] = None,\n",
    ") -> wds.DataPipeline:\n",
    "    \"\"\"\n",
    "    Load and prepare a dataset for Whisper model training.\n",
    "\n",
    "    Args:\n",
    "        dataset_dir: Path to the dataset directory\n",
    "        txt_label: Key for transcription text in dataset\n",
    "        model: Whisper model size (\"tiny\", \"base\", \"small\", \"medium\", \"large\")\n",
    "        language: Language code for tokenization\n",
    "        weight: Dataset weight for sampling\n",
    "        validation: Whether to load validation split\n",
    "        num_samples: Number of samples to load (None for all)\n",
    "\n",
    "    Returns:\n",
    "        DataPipeline: Processed dataset pipeline ready for training\n",
    "    \"\"\"\n",
    "    split = \"validation\" if validation else \"train\"\n",
    "    ds = load_dataset(dataset_dir, split=split)\n",
    "\n",
    "    if num_samples is not None:\n",
    "        ds = ds.select(range(min(num_samples, len(ds))))\n",
    "\n",
    "    adapter_ds = WhisperDatasetAdapter(ds, language, model, txt_label)\n",
    "    pipeline = wds.DataPipeline(adapter_ds, wds.shuffle(1000), wds.batched(1))\n",
    "\n",
    "    pipeline.total_samples = len(ds)\n",
    "    pipeline.weight = weight\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def load_test_dataset(\n",
    "    dataset_dir: str,\n",
    "    txt_label: str = \"transcription\",\n",
    "    model: str = \"medium\",\n",
    "    language: str = \"vi\",\n",
    "    num_samples: Optional[int] = None,\n",
    ") -> wds.DataPipeline:\n",
    "    \"\"\"\n",
    "    Load test dataset for evaluation.\n",
    "\n",
    "    Args:\n",
    "        dataset_dir: Path to the dataset directory\n",
    "        txt_label: Key for transcription text in dataset\n",
    "        model: Whisper model size\n",
    "        language: Language code for tokenization\n",
    "        num_samples: Number of samples to load (None for all)\n",
    "\n",
    "    Returns:\n",
    "        DataPipeline: Processed test dataset\n",
    "    \"\"\"\n",
    "    ds = load_dataset(dataset_dir, split=\"test\")\n",
    "\n",
    "    if num_samples is not None:\n",
    "        ds = ds.select(range(min(num_samples, len(ds))))\n",
    "\n",
    "    adapter_ds = WhisperDatasetAdapter(ds, language, model, txt_label)\n",
    "    pipeline = wds.DataPipeline(adapter_ds, wds.batched(1))\n",
    "    pipeline.total_samples = len(ds)\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "class WhisperDatasetAdapter(IterableDataset):\n",
    "    \"\"\"\n",
    "    Adapter class to process audio datasets for Whisper model training.\n",
    "\n",
    "    Handles audio preprocessing, tokenization, and formatting of inputs/outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        language: str,\n",
    "        model: str,\n",
    "        txt_label: str = \"transcription\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the dataset adapter.\n",
    "\n",
    "        Args:\n",
    "            dataset: Input dataset\n",
    "            language: Language code for tokenization\n",
    "            model: Whisper model size\n",
    "            txt_label: Key for transcription text in dataset\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.language = language\n",
    "        self.model = model\n",
    "        self.txt_label = txt_label\n",
    "        self.total_samples = len(dataset)\n",
    "        self.weight = 1.0\n",
    "        self.max_audio_length = 30 * 16000  # 30 seconds at 16kHz\n",
    "        self.token_lengths = []\n",
    "        self._analyze_token_lengths()\n",
    "\n",
    "\n",
    "    def pad_audio(self, audio: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Pad or trim audio to maximum length.\n",
    "\n",
    "        Args:\n",
    "            audio: Input audio tensor\n",
    "\n",
    "        Returns:\n",
    "            Processed audio tensor of fixed length\n",
    "        \"\"\"\n",
    "        if len(audio) > self.max_audio_length:\n",
    "            return audio[: self.max_audio_length]\n",
    "        return F.pad(audio, (0, self.max_audio_length - len(audio)), value=0)\n",
    "\n",
    "    def _process_example(\n",
    "        self, example: dict\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Process a single dataset example.\n",
    "\n",
    "        Args:\n",
    "            example: Dictionary containing audio and transcription\n",
    "\n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "                - Processed audio samples\n",
    "                - Attention mask\n",
    "                - Input tokens\n",
    "                - Output tokens\n",
    "        \"\"\"\n",
    "        # Process audio\n",
    "        audio_data = example[\"audio\"]\n",
    "        samples = torch.tensor(audio_data[\"array\"], dtype=torch.float32)\n",
    "\n",
    "        # Ensure mono audio\n",
    "        if samples.dim() == 2:\n",
    "            samples = samples.mean(0)\n",
    "\n",
    "        # Resample if needed\n",
    "        if audio_data[\"sampling_rate\"] != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(\n",
    "                audio_data[\"sampling_rate\"], 16000\n",
    "            )\n",
    "            samples = resampler(samples)\n",
    "\n",
    "        # Normalize audio\n",
    "        if samples.abs().max() > 0:\n",
    "            samples = samples / samples.abs().max()\n",
    "\n",
    "        # Pad or trim\n",
    "        samples = self.pad_audio(samples)\n",
    "\n",
    "        # Create mask for attention\n",
    "        mask = torch.zeros(30 * 16000 // 320, dtype=torch.bool)\n",
    "        audio_frames = min(len(samples), self.max_audio_length) // 320\n",
    "        mask[:audio_frames] = 1\n",
    "\n",
    "        # Process text tokens\n",
    "        text = example[self.txt_label]\n",
    "        tokenizer = whisper.tokenizer.get_tokenizer(\n",
    "            True, language=self.language, task=\"transcribe\"\n",
    "        )\n",
    "        tokens = list(tokenizer.sot_sequence_including_notimestamps) + tokenizer.encode(\n",
    "            text\n",
    "        )\n",
    "\n",
    "        # Pad tokens\n",
    "        max_tokens = 50  # TODO: don't hardcode this\n",
    "        rpad = max_tokens - len(tokens)\n",
    "\n",
    "        in_ttoks = F.pad(\n",
    "            torch.tensor(tokens, dtype=torch.long),\n",
    "            (0, rpad),\n",
    "            value=tokenizer.eot,\n",
    "        )\n",
    "        out_ttoks = F.pad(\n",
    "            torch.tensor(tokens[1:] + [tokenizer.eot], dtype=torch.long),\n",
    "            (0, rpad),\n",
    "            value=-100,\n",
    "        )\n",
    "\n",
    "        return samples, mask, in_ttoks, out_ttoks\n",
    "\n",
    "    def __iter__(\n",
    "        self,\n",
    "    ) -> Iterator[Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Iterate over the dataset.\n",
    "\n",
    "        Yields:\n",
    "            Tuple containing processed audio and text tensors\n",
    "        \"\"\"\n",
    "        for example in self.dataset:\n",
    "            try:\n",
    "                yield self._process_example(example)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping sample due to error: {e}\")\n",
    "                continue\n",
    "            \n",
    "    def _analyze_token_lengths(self):\n",
    "        \"\"\"Analyze token lengths for all examples in the dataset\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        # Iterate through the dataset with proper indexing\n",
    "        for idx in range(len(self.dataset)):\n",
    "            example = self.dataset[\"train\"][idx]\n",
    "            text = example[self.txt_label]\n",
    "            tokenizer = whisper.tokenizer.get_tokenizer(\n",
    "                True, language=self.language, task=\"transcribe\"\n",
    "            )\n",
    "            tokens = list(tokenizer.sot_sequence_including_notimestamps) + tokenizer.encode(text)\n",
    "            self.token_lengths.append(len(tokens))\n",
    "        \n",
    "        # Create visualization\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(self.token_lengths, bins=50, edgecolor='black')\n",
    "        plt.title(f'Distribution of Token Lengths (max={max(self.token_lengths)})')\n",
    "        plt.xlabel('Number of Tokens')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.axvline(x=50, color='r', linestyle='--', label='Current max_tokens (50)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Save the plot\n",
    "        plt.savefig('token_lengths_distribution.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Print some statistics\n",
    "        print(f\"Token length statistics:\")\n",
    "        print(f\"Mean: {sum(self.token_lengths) / len(self.token_lengths):.2f}\")\n",
    "        print(f\"Max: {max(self.token_lengths)}\")\n",
    "        print(f\"Min: {min(self.token_lengths)}\")\n",
    "        print(f\"Samples exceeding max_tokens (50): {sum(l > 50 for l in self.token_lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token length statistics:\n",
      "Mean: 18.33\n",
      "Max: 20\n",
      "Min: 16\n",
      "Samples exceeding max_tokens (50): 0\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"linhtran92/viet_bud500\")\n",
    "adapter = WhisperDatasetAdapter(ds, language=\"vi\", model=\"medium\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
