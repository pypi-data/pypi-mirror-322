{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bb7f461",
   "metadata": {},
   "source": [
    "# Basic usage of the ``h5pywrappers`` library #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6d9cd7",
   "metadata": {},
   "source": [
    "## A NOTE BEFORE STARTING ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880421e9",
   "metadata": {},
   "source": [
    "Since the ``h5pywrappers`` git repository tracks this notebook under its\n",
    "original basename ``basic_usage.ipynb``, we recommend that you copy the original\n",
    "notebook and rename it to any other basename that is not one of the original\n",
    "basenames that appear in the ``<root>/examples`` directory before executing any\n",
    "of the notebook cells below, where ``<root>`` is the root of the\n",
    "``h5pywrappers`` repository. This way you can explore the notebook by executing\n",
    "and modifying cells without changing the original notebook, which is being\n",
    "tracked by git."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b0d90e",
   "metadata": {},
   "source": [
    "## Import necessary modules ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f9787f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For general array handling and constructing random number generators.\n",
    "import numpy as np\n",
    "\n",
    "# For converting objects.\n",
    "import czekitout.convert\n",
    "\n",
    "\n",
    "\n",
    "# The library that is the subject of this demonstration.\n",
    "import h5pywrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e667d633",
   "metadata": {},
   "source": [
    "## Introduction ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f2509",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate how one can use each function and class in the\n",
    "``h5pywrappers`` library.\n",
    "\n",
    "You can find the documentation for the ``h5pywrappers`` library\n",
    "[here](https://mrfitzpa.github.io/h5pywrappers/_autosummary/h5pywrappers.html).\n",
    "It is recommended that you consult the documentation of this library as you\n",
    "explore the notebook. Moreover, users should execute the cells in the order that\n",
    "they appear, i.e. from top to bottom, as some cells reference variables that are\n",
    "set in other cells above them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0993ba",
   "metadata": {},
   "source": [
    "## Using the ``h5pywrappers`` function ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a2529c",
   "metadata": {},
   "source": [
    "Let's create an HDF5 file and populate it with HDF5 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2306116d-f0cb-44b5-bead-37a1ef6efbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"filename\": \"file_1.h5\", \"path_in_file\": \"/group_A/dataset_A\"}\n",
    "obj_id = h5pywrappers.obj.ID(**kwargs)\n",
    "\n",
    "float_array = np.random.rand(2, 5, 4, 6)\n",
    "\n",
    "kwargs = {\"dataset\": float_array, \"dataset_id\": obj_id, \"write_mode\": \"w\"}\n",
    "h5pywrappers.dataset.save(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0833aa4c-a421-4e4d-8d23-bfd2c7a6aaa6",
   "metadata": {},
   "source": [
    "The above notebook cell will create a new file at the file path \n",
    "``\"./file_1.h5\"``, with any file at the same file path being replaced by the new\n",
    "file. In the new file, an HDF5 dataset is located at the HDF5 path \n",
    "``\"/group_A/dataset_A\"``.\n",
    "\n",
    "If we try to re-save the HDF5 dataset when ``write_mode == \"w-\"``, an exception\n",
    "will be raised. The following notebook cell should raise an exception:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f53104e-cbd8-40ea-8efa-763584273680",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"dataset\": float_array, \"dataset_id\": obj_id, \"write_mode\": \"w-\"}\n",
    "h5pywrappers.dataset.save(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aa3fff-286f-479f-8019-0aefbb832dc3",
   "metadata": {},
   "source": [
    "Let's now load the HDF5 dataset that we just successfully saved, but in \n",
    "read-only mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d9de28-03b9-46f2-8667-cba8b05f9080",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"dataset_id\": obj_id, \"read_only\": True}\n",
    "dataset = h5pywrappers.dataset.load(**kwargs)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eb9655-b787-4af4-9903-e6593e27fea5",
   "metadata": {},
   "source": [
    "Let's check that the data is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2ff4da-e777-42da-a62f-30136000efcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bool(np.all(dataset[()] == float_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d678b89f-1983-49b5-9eac-421dc6301399",
   "metadata": {},
   "source": [
    "Since we are in read-only mode, we cannot modify the HDF5 dataset that we \n",
    "loaded. The following notebook cell should raise an exception:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5659b50d-b861-42aa-a81e-b6dd6b20c12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0, 0, 0, 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e794398-0f8f-4355-bfe8-0ad7b6db08b8",
   "metadata": {},
   "source": [
    "Let's create a new HDF5 file and populate it with the same HDF5 dataset of the\n",
    "previous HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d16295-4b2f-4f96-bb5b-f477ac617a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"filename\": \"file_2.h5\", \"path_in_file\": \"/group_A/dataset_A\"}\n",
    "obj_id = h5pywrappers.obj.ID(**kwargs)\n",
    "\n",
    "kwargs = {\"dataset\": dataset, \"dataset_id\": obj_id, \"write_mode\": \"w\"}\n",
    "h5pywrappers.dataset.save(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ace5c7-fb37-46bc-8457-88e59e56d589",
   "metadata": {},
   "source": [
    "Let's now close the first HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607adc87-4d6a-4125-9be8-2674aba81de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065f9dc2-f834-4e6e-8468-a486d1fdf2a9",
   "metadata": {},
   "source": [
    "Let's load the HDF5 dataset in the second HDF5 file, modify it, and then close \n",
    "the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa6fc03-f8f0-4ce7-ad09-9fc77d5d06e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"dataset_id\": obj_id, \"read_only\": False}\n",
    "dataset = h5pywrappers.dataset.load(**kwargs)\n",
    "dataset[0, 0, 0, 0] += 1\n",
    "dataset.file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c206641-36f9-4244-bee7-b64ec40c69f7",
   "metadata": {},
   "source": [
    "We can also store strings as HDF5 datasets. Let's append such an HDF5 dataset to\n",
    "the first HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fed17e-88eb-4e1f-869a-efb2ca825fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"filename\": \"file_1.h5\", \n",
    "          \"path_in_file\": \"/group_B/dataset_B\"}\n",
    "obj_id = h5pywrappers.obj.ID(**kwargs)\n",
    "\n",
    "string = \"hello world\"\n",
    "\n",
    "kwargs = {\"dataset\": string, \"dataset_id\": obj_id, \"write_mode\": \"a\"}\n",
    "h5pywrappers.dataset.save(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6adcc54-c92e-447c-96e2-86c388fc807c",
   "metadata": {},
   "source": [
    "Let's load the new HDF5 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d3fc28-97c0-4c88-a59c-6967c28c95e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"dataset_id\": obj_id, \"read_only\": True}\n",
    "dataset = h5pywrappers.dataset.load(**kwargs)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71234557-1ffe-4847-95ea-fbf2a589a1ce",
   "metadata": {},
   "source": [
    "Let's check that the contents of the HDF5 file are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5853cb-ee4a-4ed5-adc4-b8caf1637208",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"obj\": dataset[()], \"obj_name\": \"dataset\"}\n",
    "czekitout.convert.to_str_from_str_like(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e2d86d-6652-47f9-8ee4-196f92fefde2",
   "metadata": {},
   "source": [
    "Let's now close the HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af9bb10-a491-4d21-bac7-f3b3ca044466",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc59544f-69e4-4b00-8c0d-2969b23c6ba8",
   "metadata": {},
   "source": [
    "The last optional write mode for HDF5 datasets is toggled by setting\n",
    "``write_mode`` to ``\"a-\"``. In this mode, the HDF5 dataset of interest is saved\n",
    "unless an HDF5 object already exists at the target HDF5 path of the target HDF5\n",
    "file, in which case an error is raised. The following notebook cell should raise\n",
    "an exception:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a603a2-a269-408f-9e5d-64eef963cd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"dataset\": string, \"dataset_id\": obj_id, \"write_mode\": \"a-\"}\n",
    "h5pywrappers.dataset.save(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0667b254-9119-4a35-889d-108513941d38",
   "metadata": {},
   "source": [
    "Let's add an HDF5 attribute to one of the HDF5 groups in the first HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a328c92f-df55-474a-aedf-d07fa24d0a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"filename\": \"file_1.h5\", \"path_in_file\": \"/group_A\"}\n",
    "obj_id = h5pywrappers.obj.ID(**kwargs)\n",
    "\n",
    "attr = \"foobar\"\n",
    "attr_name = \"alternate_group_name\"\n",
    "\n",
    "kwargs = {\"obj_id\": obj_id, \"attr_name\": attr_name}\n",
    "attr_id = h5pywrappers.attr.ID(**kwargs)\n",
    "\n",
    "kwargs = {\"attr\": \"foobar\", \"attr_id\": attr_id, \"write_mode\": \"a\"}\n",
    "h5pywrappers.attr.save(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9748075-107a-4b69-a067-87b36c9366c5",
   "metadata": {},
   "source": [
    "Note that users can also save HDF5 attributes in the write mode ``\"a-\"``.\n",
    "\n",
    "Let's load the HDF5 attribute and check that it is set to the correct value.\n",
    "Note that there is no need to close the HDF5 file as it is done automatically in\n",
    "this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326919dd-333e-4c7c-97bf-da8aeacacd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"attr_id\": attr_id}\n",
    "h5pywrappers.attr.load(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb16ca24-a31a-4dfc-9df6-094ce7527b44",
   "metadata": {},
   "source": [
    "Let's create a new empty HDF5 group in the first HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862657da-78dc-4da1-aa4a-e25060a7c746",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"filename\": \"file_1.h5\", \"path_in_file\": \"/group_C\"}\n",
    "obj_id = h5pywrappers.obj.ID(**kwargs)\n",
    "\n",
    "kwargs = {\"group\": None, \"group_id\": obj_id, \"write_mode\": \"a\"}\n",
    "h5pywrappers.group.save(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56db4ce2-9bc7-446d-ac70-064c4ea5dad6",
   "metadata": {},
   "source": [
    "Note that the same writing modes are available for HDF5 groups as those of HDF5\n",
    "datasets.\n",
    "\n",
    "Let's load the newly created HDF5 group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07621eb3-b074-4057-b9a4-0ca2a50a8efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"group_id\": obj_id, \"read_only\": True}\n",
    "group = h5pywrappers.group.load(**kwargs)\n",
    "group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369b610d-32cb-4367-8d34-9dcf3837ebe3",
   "metadata": {},
   "source": [
    "Let's save the newly created HDF5 group to the second HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8722dd8-c332-4375-8d61-9fcbb9516ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"filename\": \"file_2.h5\", \"path_in_file\": \"/group_B\"}\n",
    "obj_id = h5pywrappers.obj.ID(**kwargs)\n",
    "\n",
    "kwargs = {\"group\": group, \"group_id\": obj_id, \"write_mode\": \"a-\"}\n",
    "h5pywrappers.group.save(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ed7ce3-1a1d-4ca3-a7ae-93855b1bae68",
   "metadata": {},
   "source": [
    "Let's close the first HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49903f66-ecbd-4650-9eb9-c95ef403d4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "group.file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50df6ba5-b242-42ac-bdf0-b067e85b3ce1",
   "metadata": {},
   "source": [
    "Let's save an HDF5 \"scalar\" to the first HDF5 file. By \"scalar\", we mean a\n",
    "single numerical data element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117dbc55-3d86-4c62-a0c2-da9002040cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"filename\": \"file_1.h5\", \"path_in_file\": \"/group_C/scalar_C\"}\n",
    "obj_id = h5pywrappers.obj.ID(**kwargs)\n",
    "\n",
    "scalar = 2.3+0.5j\n",
    "\n",
    "kwargs = {\"scalar\": scalar, \"scalar_id\": obj_id, \"write_mode\": \"a-\"}\n",
    "h5pywrappers.scalar.save(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da8eb96-9383-490b-95db-3239e09ab813",
   "metadata": {},
   "source": [
    "Note that the same writing modes are available for HDF5 scalars as those of HDF5\n",
    "datasets.\n",
    "\n",
    "Let's load the HDF5 scalar and check that the correct value was stored. Note \n",
    "that there is no need to close the HDF5 file as it is done automatically in this\n",
    "case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d447e4b-c489-4818-a5e6-36cfe6df6418",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"scalar_id\": obj_id}\n",
    "h5pywrappers.scalar.load(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ed23c8-e9fb-4007-aa7f-931b752f179d",
   "metadata": {},
   "source": [
    "Let's save an HDF5 \"datasubset\" to the first HDF5 file. By \"datasubset\", we mean\n",
    "an array obtained by taking a multidimensional slice of an HDF5 dataset in an\n",
    "HDF5 file. In other words, let's modify a multidimensional slice of one of the\n",
    "previously saved HDF5 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ccfbda-cf70-421d-bae1-ae3a43d6ea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"filename\": \"file_1.h5\", \"path_in_file\": \"/group_A/dataset_A\"}\n",
    "obj_id = h5pywrappers.obj.ID(**kwargs)\n",
    "\n",
    "multi_dim_slice = (1, slice(1, None, 1), [3, 0], slice(None))\n",
    "\n",
    "kwargs = {\"dataset_id\": obj_id, \"multi_dim_slice\": multi_dim_slice}\n",
    "datasubset_id = h5pywrappers.datasubset.ID(**kwargs)\n",
    "\n",
    "float_array = np.random.rand(2, 4, 6)\n",
    "\n",
    "kwargs = {\"datasubset\": float_array, \"datasubset_id\": datasubset_id}\n",
    "h5pywrappers.datasubset.save(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955e46cc-29ae-4129-82d9-d78f9e9bb5e4",
   "metadata": {},
   "source": [
    "Let's load the HDF5 datasubset and check that its contents are correct. Note \n",
    "that there is no need to close the HDF5 file as it is done automatically in this\n",
    "case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556084d7-2a30-4b53-8992-5fea09ff195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"datasubset_id\": datasubset_id}\n",
    "bool(np.all(h5pywrappers.datasubset.load(**kwargs) == float_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4ffb8a-a090-4e9d-af8d-0512e8b8eafe",
   "metadata": {},
   "source": [
    "Note that users can use alternatively ``h5pywrappers.obj.load`` to load any HDF5\n",
    "group or dataset. Let's try loading an HDF5 dataset from the first file using \n",
    "this alternative loading function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59d1094-1978-4319-bbc8-8eeca2aba84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"obj_id\": obj_id, \"read_only\": True}\n",
    "dataset = h5pywrappers.obj.load(**kwargs)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18107f36-17e0-4662-9583-e4ea0e344f62",
   "metadata": {},
   "source": [
    "Let's close the first HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89dd728-7dbb-415f-bea0-b8374911a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11778db3-e2d9-4e41-8948-8070cbf3def3",
   "metadata": {},
   "source": [
    "One of the features of ``h5pywrappers`` is that the classes \n",
    "``h5pywrappers.obj.ID``, ``h5pywrappers.attr.ID``, and\n",
    "``h5pywrappers.datasubset.ID`` are subclasses of\n",
    "``fancytypes.PreSerializableAndUpdatable``, which is a special class that is\n",
    "both updatable, and \"pre-serializable\". See [here](https://mrfitzpa.github.io/fancytypes/_autosummary/fancytypes.PreSerializableAndUpdatable.html), for a\n",
    "description of the class ``fancytypes.PreSerializableAndUpdatable``, and a\n",
    "definition of \"pre-serialization\" (as well as de-pre-serialization).\n",
    "\n",
    "Let's pre-serialize ``datasubset_id`` as a demonstration of pre-serialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28447bbf-5918-43c5-9fb3-03ab7dea6650",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_document = datasubset_id.pre_serialize()\n",
    "json_document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fd5728-9b37-4906-b747-7661363bf310",
   "metadata": {},
   "source": [
    "Let's save the above JSON document ``json_document`` to the first HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76329fb2-15e4-42ea-9c18-4a59d5b6e243",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"filename\": \"file_1.h5\", \"path_in_file\": \"/group_D/json_document_D\"}\n",
    "obj_id = h5pywrappers.obj.ID(**kwargs)\n",
    "\n",
    "kwargs = {\"json_document\": json_document, \n",
    "          \"json_document_id\": obj_id, \n",
    "          \"write_mode\": \"a-\"}\n",
    "h5pywrappers.json.document.save(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2d2ee6-72dd-4471-8cda-ddd5bcc59a78",
   "metadata": {},
   "source": [
    "Note that the same writing modes are available for JSON document as those of \n",
    "HDF5 datasets.\n",
    "\n",
    "Let's load the JSON document and check that its contents are correct. Note that\n",
    "there is no need to close the HDF5 file as it is done automatically in this \n",
    "case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cc23f2-3dac-4299-af82-0e6cfd150fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"json_document_id\": obj_id}\n",
    "h5pywrappers.json.document.load(**kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
