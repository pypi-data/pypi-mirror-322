# Copyright 2025 GlyphyAI
#
# Use of this source code is governed by an MIT-style
# license that can be found in the LICENSE file or at
# https://opensource.org/licenses/MIT.

from typing import Any, Generic, Literal, Self, TypeAlias

from litellm import ChatCompletionAudioParam, ChatCompletionModality, ChatCompletionPredictionContentParam, ModelConfig
from litellm.types.utils import ChatCompletionAudioResponse, ChatCompletionDeltaToolCall, FunctionCall
from litellm.types.utils import Delta as LiteDelta
from litellm.types.utils import Usage as LiteUsage
from pydantic import BaseModel, Field, field_serializer, model_validator
from typing_extensions import TypedDict, TypeVar

from liteswarm.types.base import SwarmBaseModel

MessageRole: TypeAlias = Literal["assistant", "user", "system", "tool", "function", "developer"]
"""Role of the message author in a conversation.

Supported roles:
- assistant: Model-generated responses to user messages.
- user: End-user messages containing prompts or context.
- system: System-level instructions that guide model behavior.
- tool: Results from tool/function executions.
- function: Legacy alias for tool responses [deprecated].
- developer: Developer-provided instructions for reasoning models.
"""

ToolCall: TypeAlias = ChatCompletionDeltaToolCall
"""Tool call in a streaming response from the model.

Represents a function or tool invocation request with:
- id: Unique identifier for tracking the call.
- function: Function name and arguments to execute.
- type: Optional tool type identifier (e.g., 'function').
- index: Position in the sequence of tool calls.

Used in both complete messages and streaming deltas.
"""

AudioResponse: TypeAlias = ChatCompletionAudioResponse
"""Audio response generated by the model.

Contains audio data and metadata:
- id: Unique identifier for the response.
- data: Base64 encoded audio bytes in requested format.
- expires_at: Unix timestamp when audio will be purged.
- transcript: Text transcription of the audio content.

Used in conversations with audio capabilities.
"""

Usage: TypeAlias = LiteUsage
"""Usage information for a model response.

Tracks token usage for both input prompts and model completions.
"""

FinishReason: TypeAlias = Literal["stop", "length", "tool_calls", "function_call", "content_filter"]
"""OpenAI supported finish reasons for streaming responses.

Indicates why the model stopped generating tokens during a streaming response.

Supported finish reasons:
- stop: Model reached a natural stopping point or hit a provided stop sequence.
- length: Maximum allowed tokens were reached.
- tool_calls: Model called a tool.
- function_call: Deprecated equivalent of tool_calls.
- content_filter: Content was omitted due to safety or moderation filters.
"""


class ToolChoiceFunctionObject(TypedDict):
    """Function specification for tool choice.

    Defines the specific function an agent should use when multiple
    tools are available.
    """

    name: str
    """Name of the function to use."""


class ToolChoiceFunction(TypedDict):
    """Complete tool choice specification.

    Combines the choice type with the specific function to use.
    """

    type: Literal["function"]
    """Type of tool choice (always "function")."""

    function: ToolChoiceFunctionObject
    """Function specification."""


ToolChoice: TypeAlias = Literal["auto", "none", "required"] | ToolChoiceFunction
"""Specification for how an agent should use tools.

Controls tool selection and usage behavior:
- "auto": Agent decides when to use tools.
- "none": Agent cannot use tools.
- "required": Agent must use a tool.
- ToolChoiceFunction: Agent must use specific tool.

Examples:
    Automatic tool selection:
        ```python
        llm = LLM(
            model="gpt-4o",
            tool_choice="auto"
        )
        ```

    Force specific tool:
        ```python
        llm = LLM(
            model="gpt-4o",
            tool_choice={
                "type": "function",
                "function": {"name": "calculate"}
            }
        )
        ```
"""


class Message(SwarmBaseModel):
    """Message in a conversation between user, assistant, and tools.

    Represents a message in a conversation between participants
    with content and optional tool interactions. Each message has
    a specific role and may include tool calls or responses.

    Examples:
        System message:
            ```python
            system_msg = Message(
                role="system",
                content="You are a helpful assistant.",
            )
            ```

        Assistant with tool:
            ```python
            assistant_msg = Message(
                role="assistant",
                content="Let me calculate that.",
                tool_calls=[
                    ToolCall(
                        id="calc_1",
                        function={"name": "add", "arguments": '{"a": 2, "b": 2}'},
                        type="function",
                        index=0,
                    )
                ],
            )
            ```

        Tool response:
            ```python
            tool_msg = Message(
                role="tool",
                content="4",
                tool_call_id="calc_1",
            )
            ```
    """

    role: MessageRole
    """Role of the message author."""

    content: str | None = None
    """Text content of the message."""

    tool_calls: list[ToolCall] | None = None
    """Tool calls made in this message."""

    tool_call_id: str | None = None
    """ID of the tool call this message responds to."""

    audio: AudioResponse | None = None
    """Audio response data if available."""


class Delta(SwarmBaseModel):
    """Streaming update from language model generation.

    Contains new content and changes since the last update during
    streaming. Updates can include text content, role changes,
    tool calls, or audio responses.

    Note:
        Any field may be empty or partially complete.
    """

    content: str | None = None
    """New text content."""

    role: MessageRole | None = None
    """Role of the message author."""

    function_call: FunctionCall | None = None
    """Function call update (deprecated)."""

    tool_calls: list[ToolCall] | None = None
    """Tool calls being made."""

    audio: AudioResponse | None = None
    """Audio response if available."""

    @property
    def is_empty(self) -> bool:
        """Check if the delta is empty.

        Returns:
            True if the delta is empty, False otherwise.
        """
        return all(not getattr(self, attr) for attr in self.model_fields)

    @classmethod
    def from_delta(cls, delta: LiteDelta) -> "Delta":
        """Create a Delta from a LiteLLM delta object.

        Args:
            delta: LiteLLM delta to convert.

        Returns:
            New Delta instance with copied attributes.
        """
        return cls(
            content=delta.content,
            role=delta.role,
            function_call=delta.function_call,
            tool_calls=delta.tool_calls,
            audio=delta.audio,
        )


class ResponseCost(SwarmBaseModel):
    """Cost information for a model response.

    Tracks token costs for both input prompts and model completions.
    """

    prompt_tokens_cost: float
    """Cost of input tokens."""

    completion_tokens_cost: float
    """Cost of output tokens."""

    total_tokens_cost: float
    """Total cost of the response."""


class ResponseSchema(SwarmBaseModel):
    """Schema for validating structured responses.

    Defines the expected structure and validation rules for model outputs.

    Examples:
        Define review output schema:
            ```python
            schema = ResponseSchema(
                name="review_output",
                description="Code review response format",
                json_schema={
                    "type": "object",
                    "required": ["approved", "comments"],
                    "properties": {
                        "approved": {"type": "boolean"},
                        "comments": {"type": "array", "items": {"type": "string"}},
                    },
                },
            )
            ```
    """

    name: str
    """Schema identifier."""

    description: str | None = None
    """Schema purpose and usage."""

    json_schema: dict[str, Any] | None = Field(default=None, alias="schema")
    """JSON schema definition."""

    strict: bool = False
    """Whether to enforce strict validation."""


class ResponseFormatText(TypedDict):
    """Text response format specification."""

    type: Literal["text"]
    """Response format type."""


class ResponseFormatJsonObject(TypedDict):
    """JSON object response format specification."""

    type: Literal["json_object"]
    """Response format type."""


class ResponseFormatJsonSchema(TypedDict):
    """JSON schema response format specification."""

    type: Literal["json_schema"]
    """Response format type."""

    json_schema: ResponseSchema
    """Schema for validation."""


class StreamOptions(SwarmBaseModel):
    """Configuration for streaming responses.

    Controls what additional information is included in streaming
    response chunks.
    """

    include_usage: bool | None = None
    """Whether to include token usage stats."""


ResponseFormatPydantic = TypeVar("ResponseFormatPydantic", bound=BaseModel, default=BaseModel)
"""Type variable for Pydantic model used as response output.

This type variable allows for type-safe validation of response formats
by ensuring that the response format is a subclass of BaseModel.
"""

ResponseFormat: TypeAlias = (
    ResponseFormatText  # noop
    | ResponseFormatJsonObject
    | ResponseFormatJsonSchema
    | type[ResponseFormatPydantic]
)
"""Specification for model output format and validation.

Controls response structure and validation:
- ResponseFormatBasic: Text or generic JSON.
- ResponseFormatJsonSchema: Validated against JSON schema.
- type[BaseModel]: Validated against Pydantic model.

Examples:
    Text response:
        ```python
        llm = LLM(
            model="gpt-4o",
            response_format={"type": "text"}
        )
        ```

    JSON schema validation:
        ```python
        llm = LLM(
            model="gpt-4o",
            response_format={
                "type": "json_schema",
                "json_schema": ResponseSchema(
                    name="review",
                    json_schema={
                        "type": "object",
                        "properties": {
                            "approved": {"type": "boolean"},
                            "comments": {"type": "array"}
                        }
                    }
                )
            }
        )
        ```

    Pydantic model validation:
        ```python
        class ReviewOutput(BaseModel):
            approved: bool
            comments: list[str]

        llm = LLM(
            model="gpt-4o",
            response_format=ReviewOutput
        )
        ```
"""


class LLM(SwarmBaseModel, Generic[ResponseFormatPydantic]):
    """Configuration for language model interactions.

    Provides comprehensive control over LLM behavior including model
    selection, response formatting, tool usage, and performance settings.

    Examples:
        Basic configuration:
            ```python
            llm = LLM(
                model="gpt-4o",
                max_tokens=1000,
                temperature=0.7,
            )
            ```

        Advanced configuration:
            ```python
            class ReviewOutput(BaseModel):
                approved: bool
                comments: list[str]


            llm = LLM(
                model="gpt-4o",
                tool_choice="auto",
                parallel_tool_calls=True,
                response_format=ReviewOutput,
                temperature=0.7,
                stream_options=StreamOptions(include_usage=True),
            )
            ```
    """

    model: str
    """Model identifier."""

    tool_choice: ToolChoice | None = None
    """Tool selection behavior."""

    parallel_tool_calls: bool | None = None
    """Allow concurrent tool calls."""

    response_format: ResponseFormat[ResponseFormatPydantic] | None = None
    """Output format specification."""

    logprobs: bool | None = None
    """Include token logprobs."""

    top_logprobs: int | None = None
    """Number of top logprobs."""

    deployment_id: str | None = None
    """Model deployment identifier."""

    seed: int | None = None
    """Random seed for reproducibility."""

    user: str | None = None
    """User identifier for tracking."""

    logit_bias: dict[int, float] | None = None
    """Token probability adjustments."""

    frequency_penalty: float | None = None
    """Repetition reduction factor."""

    presence_penalty: float | None = None
    """Topic diversity factor."""

    max_tokens: int | None = None
    """Maximum response length."""

    max_completion_tokens: int | None = None
    """Maximum completion length."""

    modalities: list[ChatCompletionModality] | None = None
    """Response modality settings."""

    prediction: ChatCompletionPredictionContentParam | None = None
    """Prediction output settings."""

    audio: ChatCompletionAudioParam | None = None
    """Audio generation settings."""

    stop: str | list[str] | None = None
    """Response termination sequences."""

    stream_options: StreamOptions | None = None
    """Streaming configuration."""

    stream: bool | None = None
    """Enable response streaming."""

    n: int | None = None
    """Number of completions."""

    top_p: float | None = None
    """Nucleus sampling threshold."""

    temperature: float | None = None
    """Response randomness."""

    timeout: int | float | None = None
    """Request timeout seconds."""

    litellm_kwargs: dict[str, Any] | None = None
    """Additional LiteLLM options."""

    base_url: str | None = None
    """Base URL for API requests."""

    api_version: str | None = None
    """API version string."""

    api_key: str | None = None
    """API authentication key."""

    model_list: list[ModelConfig] | None = None
    """List of available model configurations."""

    extra_headers: dict[str, Any] | None = None
    """Additional HTTP headers to include in API requests."""

    @field_serializer("response_format")
    def serialize_response_format(
        self,
        response_format: ResponseFormat[ResponseFormatPydantic] | None,
    ) -> dict[str, Any] | None:
        """Serialize response format for API requests.

        Handles different response format types:
        - ResponseFormatBasic: Pass through (e.g., {"type": "text"})
        - ResponseFormatJsonSchema: Pass through with schema
        - type[BaseModel]: Convert to JSON schema

        Args:
            response_format: Format to serialize.

        Returns:
            API-compatible format specification or None.
        """
        if isinstance(response_format, type) and issubclass(response_format, BaseModel):
            return response_format.model_json_schema()
        elif isinstance(response_format, dict):
            return {**response_format}
        else:
            return None

    @model_validator(mode="after")
    def check_litellm_kwargs_keys(self) -> Self:
        """Validate litellm_kwargs against main configuration.

        Ensures additional kwargs don't conflict with explicitly set fields.

        Raises:
            ValueError: If litellm_kwargs contains conflicting keys.
        """
        if self.litellm_kwargs:
            field_names = set(self.model_fields.keys()) - {"litellm_kwargs"}
            overlapping_keys = field_names.intersection(self.litellm_kwargs.keys())
            if overlapping_keys:
                raise ValueError(
                    f"litellm_kwargs contains keys that are already defined in LLM: {', '.join(overlapping_keys)}"
                )

        return self
