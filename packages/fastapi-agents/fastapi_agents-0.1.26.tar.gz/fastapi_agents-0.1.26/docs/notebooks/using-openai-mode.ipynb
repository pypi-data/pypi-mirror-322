{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using OpenAI Mode\n",
    "\n",
    "This notebook walks you through enabling OpenAI mode to serve a PydanticAI agent, and connecting to it from the OpenAI SDK.\n",
    "\n",
    "To run this notebook, make sure you have `fastapi`, `fastapi-agents`, `pydantic-ai` and `openai` installed (e.g. `pip install fastapi fastapi-agents pydantic-ai openai`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your agent with FastAPI Agents\n",
    "\n",
    "We're going to use a simple weather agent for the demo. The weather is always great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi_agents import FastAPIAgents\n",
    "from fastapi_agents.pydantic_ai import PydanticAIAgent\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "weather_agent = Agent(\"openai:gpt-4o\")\n",
    "\n",
    "@weather_agent.tool\n",
    "async def get_weather(_, location: str) -> str:\n",
    "    \"\"\"Check the weather for a given location.\n",
    "    \n",
    "    Args:\n",
    "        location (str): The city to check the weather\n",
    "    \"\"\"\n",
    "    return f\"The weather is great in {location}\"\n",
    "\n",
    "# IMPORTANT: mode = 'openai'\n",
    "app = FastAPIAgents.as_app(mode='openai')\n",
    "app.register(\"weather\", PydanticAIAgent(weather_agent))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run our API in the background\n",
    "\n",
    "Since we're in Jupyter, we're going to run our API as a thread so we can execute the cells following it. We also need to nest asyncio since Jupyter is already running its own asyncio event loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import threading\n",
    "from uvicorn import Config, Server\n",
    "\n",
    "# Apply nest_asyncio to handle event loop issues in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Define a server instance\n",
    "config = Config(app=app, log_level=\"info\")\n",
    "server = Server(config)\n",
    "\n",
    "# Function to run the server\n",
    "def start_server():\n",
    "    server.run()\n",
    "\n",
    "# Start the server in a background thread\n",
    "thread = threading.Thread(target=start_server, daemon=True)\n",
    "thread.start()\n",
    "\n",
    "print(\"Uvicorn server is running in the background\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting OpenAI SDK\n",
    "\n",
    "The OpenAI SDK allows us to change the base_url for requests, so we're going to use the FastAPI local server we just started.\n",
    "\n",
    "Let's check our the agent we registered is available!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=f'http://{server.config.host}:{server.config.port}')\n",
    "\n",
    "for model in client.models.list():\n",
    "    model_id = model.id\n",
    "    break\n",
    "\n",
    "print(f\"Found {model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a chat completion\n",
    "\n",
    "We can interact with our agent as if it were an OpenAI model by using the chat completions API.\n",
    "\n",
    "Just specify the correct model ID and messages as usual.\n",
    "\n",
    "Please note that OpenAI supports many different functions in their official API, most of which are not currently available in FastAPI Agents.\n",
    "\n",
    "If you have a specific request, please open an issue on GitHub with the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=model_id,\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"what is the weather in New York?\"\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "\n",
    "When we're done, we can easily stop the server we were running in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the server\n",
    "server.should_exit = True\n",
    "thread.join()  # Wait for the thread to finish\n",
    "\n",
    "print(\"Uvicorn server has been stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
